{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-02-10T12:24:31.917515Z"
    }
   },
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def generate_attribute_prompts():\n",
    "    \"\"\"Generate text prompts for all attributes\"\"\"\n",
    "    prompts = {\n",
    "        'hair_color': {\n",
    "            0: \"a photo of a person with blonde hair\",\n",
    "            1: \"a photo of a person with brown hair\",\n",
    "            2: \"a photo of a person with black hair\",\n",
    "            3: \"a photo of a person with other hair color\"\n",
    "        },\n",
    "        'pale_skin': {\n",
    "            1: \"a photo of a person with pale skin\",\n",
    "            -1: \"a photo of a person with non-pale skin\"\n",
    "        },\n",
    "        'male': {\n",
    "            1: \"a photo of a man\",\n",
    "            -1: \"a photo of a woman\"\n",
    "        },\n",
    "        'no_beard': {\n",
    "            1: \"a photo of a person without a beard\",\n",
    "            -1: \"a photo of a person with a beard\"\n",
    "        }\n",
    "    }\n",
    "    return prompts\n",
    "\n",
    "def evaluate_attributes(image_path, attribute_prompts, model, preprocess, device):\n",
    "    \"\"\"Evaluate a single image for all attributes using CLIP\"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get image features\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Process each attribute\n",
    "        for attr, prompts in attribute_prompts.items():\n",
    "            # Prepare text inputs\n",
    "            text_inputs = clip.tokenize([prompt for prompt in prompts.values()]).to(device)\n",
    "            \n",
    "            # Get text features\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            \n",
    "            # Get prediction (index of highest similarity)\n",
    "            pred_idx = similarity[0].argmax().item()\n",
    "            \n",
    "            # Map back to original label\n",
    "            pred_label = list(prompts.keys())[pred_idx]\n",
    "            results[attr] = pred_label\n",
    "            \n",
    "    return results\n",
    "\n",
    "def evaluate_clip_performance(df, image_dir):\n",
    "    \"\"\"Evaluate CLIP's performance on the dataset\"\"\"\n",
    "    # Generate prompts\n",
    "    attribute_prompts = generate_attribute_prompts()\n",
    "    \n",
    "    # Initialize results\n",
    "    results = {\n",
    "        'hair_color': {'correct': 0, 'total': 0, 'per_class': {}},\n",
    "        'pale_skin': {'correct': 0, 'total': 0, 'per_class': {}},\n",
    "        'male': {'correct': 0, 'total': 0, 'per_class': {}},\n",
    "        'no_beard': {'correct': 0, 'total': 0, 'per_class': {}}\n",
    "    }\n",
    "    \n",
    "    # Initialize per-class counters\n",
    "    for attr in results:\n",
    "        for label in attribute_prompts[attr].keys():\n",
    "            results[attr]['per_class'][label] = {'correct': 0, 'total': 0}\n",
    "    \n",
    "    # Process each image\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
    "        image_path = os.path.join(image_dir, row['image_id'])\n",
    "        \n",
    "        try:\n",
    "            # Get predictions for all attributes\n",
    "            predictions = evaluate_attributes(image_path, attribute_prompts, model, preprocess, device)\n",
    "            \n",
    "            # Update results\n",
    "            for attr, pred in predictions.items():\n",
    "                # Map DataFrame columns to attribute names\n",
    "                column_mapping = {\n",
    "                    'hair_color': 'Hair_Color',\n",
    "                    'pale_skin': 'Pale_Skin',\n",
    "                    'male': 'Male',\n",
    "                    'no_beard': 'No_Beard'\n",
    "                }\n",
    "                \n",
    "                true_label = row[column_mapping[attr]]\n",
    "                \n",
    "                # Update overall counts\n",
    "                results[attr]['total'] += 1\n",
    "                if pred == true_label:\n",
    "                    results[attr]['correct'] += 1\n",
    "                \n",
    "                # Update per-class counts\n",
    "                results[attr]['per_class'][true_label]['total'] += 1\n",
    "                if pred == true_label:\n",
    "                    results[attr]['per_class'][true_label]['correct'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    accuracies = {}\n",
    "    per_class_accuracies = {}\n",
    "    \n",
    "    for attr, data in results.items():\n",
    "        # Overall accuracy\n",
    "        accuracies[attr] = data['correct'] / data['total'] if data['total'] > 0 else 0\n",
    "        \n",
    "        # Per-class accuracy\n",
    "        per_class_accuracies[attr] = {}\n",
    "        for label, counts in data['per_class'].items():\n",
    "            acc = counts['correct'] / counts['total'] if counts['total'] > 0 else 0\n",
    "            per_class_accuracies[attr][label] = acc\n",
    "    \n",
    "    return accuracies, per_class_accuracies\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the DataFrame\n",
    "    df = pd.read_csv(r\"/home/omrid/Desktop/jungo /projectCLIPvae/celeba_dataset/list_attr_celeba.csv\")\n",
    "    \n",
    "    # Define hair color mapping function\n",
    "    def haircolor(x):\n",
    "        if x[\"Blond_Hair\"] == 1:\n",
    "            return 0\n",
    "        elif x[\"Brown_Hair\"] == 1:\n",
    "            return 1\n",
    "        elif x[\"Black_Hair\"] == 1:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    \n",
    "    # Apply hair color mapping and select relevant columns\n",
    "    df[\"Hair_Color\"] = df.apply(haircolor, axis=1)\n",
    "    df = df[[\"image_id\", \"Hair_Color\", 'Pale_Skin', \"Male\", \"No_Beard\"]]\n",
    "    \n",
    "    # Take only first 10000 images\n",
    "    df = df.head(10000)\n",
    "    \n",
    "    # Set your image directory\n",
    "    image_dir = \"/home/omrid/Desktop/jungo /projectCLIPvae/celeba_dataset/img_align_celeba/img_align_celeba/\"\n",
    "    \n",
    "    # Evaluate CLIP\n",
    "    accuracies, per_class_accuracies = evaluate_clip_performance(df, image_dir)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nOverall Accuracies:\")\n",
    "    for attr, acc in accuracies.items():\n",
    "        print(f\"{attr}: {acc:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class Accuracies:\")\n",
    "    for attr, class_accs in per_class_accuracies.items():\n",
    "        print(f\"\\n{attr}:\")\n",
    "        for cls, acc in class_accs.items():\n",
    "            print(f\"  Class {cls}: {acc:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T12:24:42.720960Z",
     "start_time": "2025-02-10T12:24:38.963398Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9696a1d96e22dea7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omrid/miniconda3/envs/clipVAE/lib/python3.11/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/celeba/images'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Classify images\u001B[39;00m\n\u001B[1;32m     16\u001B[0m img_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/path/to/celeba/images\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 17\u001B[0m results_df \u001B[38;5;241m=\u001B[39m \u001B[43mclassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassify_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_attributes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Prepare for VAE\u001B[39;00m\n\u001B[1;32m     20\u001B[0m vae_ready_df \u001B[38;5;241m=\u001B[39m prepare_attributes_for_vae(results_df, custom_attributes)\n",
      "File \u001B[0;32m~/Desktop/jungo /projectCLIPvae/clip_classifier.py:67\u001B[0m, in \u001B[0;36mCustomAttributeClassifier.classify_images\u001B[0;34m(self, img_dir, attribute_values, batch_size)\u001B[0m\n\u001B[1;32m     64\u001B[0m templates \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_text_templates(attribute_values)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;66;03m# Get list of image files\u001B[39;00m\n\u001B[0;32m---> 67\u001B[0m image_files \u001B[38;5;241m=\u001B[39m [f \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_dir\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m f\u001B[38;5;241m.\u001B[39mendswith((\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.jpg\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.jpeg\u001B[39m\u001B[38;5;124m'\u001B[39m))]\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# Initialize results dictionary\u001B[39;00m\n\u001B[1;32m     70\u001B[0m results \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage_id\u001B[39m\u001B[38;5;124m'\u001B[39m: image_files,\n\u001B[1;32m     72\u001B[0m }\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/path/to/celeba/images'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cc90c04d4d7d3ada"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T12:24:16.632960Z",
     "start_time": "2025-02-10T12:24:16.621912Z"
    }
   },
   "cell_type": "code",
   "source": "print(fsdf)",
   "id": "94f657996dfd908c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fsdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mfsdf\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'fsdf' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "283b397b85119e74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
