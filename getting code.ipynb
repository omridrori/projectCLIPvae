{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T18:40:00.065601Z",
     "start_time": "2025-02-16T18:39:59.743980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def generate_py_file_string(directory=\".\"):\n",
    "    result = \"\"\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    result += f\"File: {file}\\n\\n{content}\\n{'-' * 40}\\n\"\n",
    "                except Exception as e:\n",
    "                    result += f\"File: {file}\\n\\nError reading file: {e}\\n{'-' * 40}\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output = generate_py_file_string()\n",
    "    print(output)\n",
    "    "
   ],
   "id": "e3f36a820c575835",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: models.py\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "from torch import nn\n",
      "import torch.nn.functional as F\n",
      "from clip_loss import CLIPAttributeConsistency\n",
      "\n",
      "\n",
      "class Encoder(nn.Module):\n",
      "    def __init__(self, latent_dim=64):\n",
      "        super(Encoder, self).__init__()\n",
      "        self.latent_dim = latent_dim\n",
      "\n",
      "        # Convolutional layers for feature extraction\n",
      "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1)\n",
      "        self.conv2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
      "        self.conv3 = nn.Conv2d(64, 64, 4, 2, 1)\n",
      "        self.conv4 = nn.Conv2d(64, 128, 4, 2, 1)\n",
      "\n",
      "        # Pooling layers\n",
      "        self.maxp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "\n",
      "        # FC layers for mu and logvar with dynamic latent dimension\n",
      "        self.fc_mu = nn.Linear(128, latent_dim)\n",
      "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # Feature extraction\n",
      "        out = self.conv1(x)\n",
      "        out = self.maxp1(out)\n",
      "        out = F.relu(out)\n",
      "\n",
      "        out = self.conv2(out)\n",
      "        out = self.maxp2(out)\n",
      "        out = F.relu(out)\n",
      "\n",
      "        out = self.conv3(out)\n",
      "        out = self.maxp3(out)\n",
      "        out = F.relu(out)\n",
      "\n",
      "        out = self.conv4(out)\n",
      "        out = self.maxp4(out)\n",
      "        out = F.relu(out)\n",
      "\n",
      "        # Flatten for FC layers\n",
      "        out = out.view(out.size(0), -1)\n",
      "\n",
      "        # Generate latent distribution parameters\n",
      "        mu = self.fc_mu(out)\n",
      "        logvar = self.fc_logvar(out)\n",
      "\n",
      "        return mu, logvar\n",
      "\n",
      "\n",
      "class Decoder(nn.Module):\n",
      "    def __init__(self, attribute_dims, latent_dim=64, embedding_dim=10):\n",
      "        \"\"\"\n",
      "        Initialize decoder with dynamic attribute dimensions\n",
      "\n",
      "        Args:\n",
      "            attribute_dims: Dictionary mapping attribute names to number of possible values\n",
      "                          e.g., {\"ethnicity\": 3, \"facial_expression\": 3}\n",
      "            latent_dim: Dimension of the latent space\n",
      "            embedding_dim: Dimension of each attribute embedding\n",
      "        \"\"\"\n",
      "        super(Decoder, self).__init__()\n",
      "\n",
      "        self.latent_dim = latent_dim\n",
      "        self.embedding_dim = embedding_dim\n",
      "        self.attribute_dims = attribute_dims\n",
      "\n",
      "        # Create embeddings for each attribute\n",
      "        self.embeddings = nn.ModuleDict({\n",
      "            attr_name: nn.Embedding(num_values, embedding_dim)\n",
      "            for attr_name, num_values in attribute_dims.items()\n",
      "        })\n",
      "\n",
      "        # Calculate total input dimension (latent + all attribute embeddings)\n",
      "        total_embedding_dims = len(attribute_dims) * embedding_dim\n",
      "        self.total_input_dim = latent_dim + total_embedding_dims\n",
      "\n",
      "        # Transposed convolution layers\n",
      "        self.transconv1 = nn.ConvTranspose2d(self.total_input_dim, 64, 8, 4, 2)\n",
      "        self.transconv2 = nn.ConvTranspose2d(64, 64, 8, 4, 2)\n",
      "        self.transconv3 = nn.ConvTranspose2d(64, 64, 4, 2, 1)\n",
      "        self.transconv4 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
      "\n",
      "        # Store attribute names in order\n",
      "        self.attribute_names = list(attribute_dims.keys())\n",
      "\n",
      "    def forward(self, x):\n",
      "        \"\"\"\n",
      "        Forward pass with dynamic attribute handling\n",
      "\n",
      "        Args:\n",
      "            x: Concatenated tensor of [latent_vector, attr1, attr2, ...]\n",
      "        \"\"\"\n",
      "        # Split latent vector and attributes\n",
      "        z = x[:, :self.latent_dim]\n",
      "        current_idx = self.latent_dim\n",
      "\n",
      "        # Process each attribute through its embedding\n",
      "        embeddings = []\n",
      "        for attr_name in self.attribute_names:\n",
      "            attr_val = x[:, current_idx].long()\n",
      "            embedding = self.embeddings[attr_name](attr_val)\n",
      "            embeddings.append(embedding)\n",
      "            current_idx += 1\n",
      "\n",
      "        # Concatenate latent vector with all embeddings\n",
      "        z = torch.cat([z] + embeddings, dim=1)\n",
      "\n",
      "        # Reshape for transposed convolutions\n",
      "        z = z.view(z.shape[0], z.shape[1], 1, 1)\n",
      "\n",
      "        # Decode\n",
      "        out = F.relu(self.transconv1(z))\n",
      "        out = F.relu(self.transconv2(out))\n",
      "        out = F.relu(self.transconv3(out))\n",
      "        out = torch.sigmoid(self.transconv4(out))\n",
      "\n",
      "        return out\n",
      "\n",
      "\n",
      "class CVAE(nn.Module):\n",
      "    def __init__(self, attribute_dims, latent_dim=64, embedding_dim=10):\n",
      "        \"\"\"\n",
      "        Initialize CVAE with dynamic attributes\n",
      "\n",
      "        Args:\n",
      "            attribute_dims: Dictionary mapping attribute names to number of possible values\n",
      "            latent_dim: Dimension of the latent space\n",
      "            embedding_dim: Dimension of each attribute embedding\n",
      "        \"\"\"\n",
      "        super(CVAE, self).__init__()\n",
      "\n",
      "        self.encoder = Encoder(latent_dim)\n",
      "        self.decoder = Decoder(attribute_dims, latent_dim, embedding_dim)\n",
      "        self.latent_dim = latent_dim\n",
      "\n",
      "    def forward(self, x, attrs):\n",
      "        # Get latent distribution parameters\n",
      "        mu, logvar = self.encoder(x)\n",
      "\n",
      "        # Reparameterization trick\n",
      "        if self.training:\n",
      "            std = torch.exp(0.5 * logvar)\n",
      "            eps = torch.randn_like(std)\n",
      "            z = mu + eps * std\n",
      "        else:\n",
      "            z = mu\n",
      "\n",
      "        # Concatenate with attributes\n",
      "        z = torch.cat([z, attrs], dim=1)\n",
      "\n",
      "        # Decode\n",
      "        out = self.decoder(z)\n",
      "        return out, mu, logvar\n",
      "\n",
      "\n",
      "def loss_function(recon_x, x, mu, logvar, clip_consistency=None, attrs=None,\n",
      "                  attribute_names=None, beta_vae=1.0, beta_clip=1.0):\n",
      "    \"\"\"\n",
      "    Compute VAE loss with optional CLIP consistency\n",
      "\n",
      "    Args:\n",
      "        recon_x: Reconstructed image\n",
      "        x: Original image\n",
      "        mu: Mean of latent distribution\n",
      "        logvar: Log variance of latent distribution\n",
      "        clip_consistency: Optional CLIPAttributeConsistency instance\n",
      "        attrs: Attribute tensor (required if clip_consistency is provided)\n",
      "        attribute_names: List of attribute names in order\n",
      "        beta_vae: Weight for KL divergence term\n",
      "        beta_clip: Weight for CLIP consistency loss\n",
      "    \"\"\"\n",
      "    # Reconstruction loss\n",
      "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
      "    recon_loss = recon_loss / x.size(0)\n",
      "\n",
      "    # KL divergence\n",
      "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
      "    kld = kld / x.size(0)\n",
      "\n",
      "    # Initialize total loss\n",
      "    total_loss = recon_loss + beta_vae * kld\n",
      "\n",
      "    # Add CLIP consistency loss if provided\n",
      "    clip_loss = 0\n",
      "    if clip_consistency is not None and attrs is not None and attribute_names is not None:\n",
      "        clip_loss = clip_consistency.compute_attribute_loss(recon_x, attrs, attribute_names)\n",
      "        total_loss += beta_clip * clip_loss\n",
      "\n",
      "    return total_loss, recon_loss, kld, clip_loss\n",
      "----------------------------------------\n",
      "File: utils.py\n",
      "\n",
      "import torch\n",
      "import pandas as pd\n",
      "from typing import Dict, List, Union, Tuple\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def validate_attribute_config(attribute_dims: Dict[str, int]) -> bool:\n",
      "    \"\"\"\n",
      "    Validate attribute configuration dictionary.\n",
      "\n",
      "    Args:\n",
      "        attribute_dims: Dictionary mapping attribute names to number of possible values\n",
      "\n",
      "    Returns:\n",
      "        bool: True if configuration is valid\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If configuration is invalid\n",
      "    \"\"\"\n",
      "    if not attribute_dims:\n",
      "        raise ValueError(\"attribute_dims cannot be empty\")\n",
      "\n",
      "    for attr_name, num_values in attribute_dims.items():\n",
      "        if not isinstance(num_values, int):\n",
      "            raise ValueError(f\"Number of values for {attr_name} must be integer\")\n",
      "        if num_values < 2:\n",
      "            raise ValueError(f\"Attribute {attr_name} must have at least 2 values\")\n",
      "\n",
      "    return True\n",
      "\n",
      "\n",
      "def convert_clip_to_vae_format(clip_df: pd.DataFrame,\n",
      "                               attribute_names: List[str]) -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
      "    \"\"\"\n",
      "    Convert CLIP classifier output to VAE-compatible format.\n",
      "\n",
      "    Args:\n",
      "        clip_df: DataFrame from CLIP classifier\n",
      "        attribute_names: List of attribute names to process\n",
      "\n",
      "    Returns:\n",
      "        Tuple containing:\n",
      "            - DataFrame with processed attributes\n",
      "            - Dictionary of attribute dimensions\n",
      "    \"\"\"\n",
      "    vae_df = clip_df.copy()\n",
      "    attribute_dims = {}\n",
      "\n",
      "    for attr in attribute_names:\n",
      "        if attr not in vae_df.columns:\n",
      "            raise ValueError(f\"Attribute {attr} not found in DataFrame\")\n",
      "\n",
      "        # Get number of unique values\n",
      "        unique_values = sorted(vae_df[attr].unique())\n",
      "        attribute_dims[attr] = len(unique_values)\n",
      "\n",
      "        # Ensure values are zero-based indices\n",
      "        value_map = {val: idx for idx, val in enumerate(unique_values)}\n",
      "        vae_df[attr] = vae_df[attr].map(value_map)\n",
      "\n",
      "    return vae_df, attribute_dims\n",
      "\n",
      "\n",
      "def create_attribute_noise(batch_size: int,\n",
      "                           attribute_dims: Dict[str, int],\n",
      "                           device: str = \"cuda\") -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Create random attribute tensor for generation.\n",
      "\n",
      "    Args:\n",
      "        batch_size: Number of samples to generate\n",
      "        attribute_dims: Dictionary of attribute dimensions\n",
      "        device: Device to create tensor on\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Random attribute tensor\n",
      "    \"\"\"\n",
      "    attrs = []\n",
      "    for num_values in attribute_dims.values():\n",
      "        attr = torch.randint(0, num_values, (batch_size,), device=device)\n",
      "        attrs.append(attr)\n",
      "    return torch.stack(attrs, dim=1)\n",
      "\n",
      "\n",
      "def interpolate_attributes(start_attrs: torch.Tensor,\n",
      "                           end_attrs: torch.Tensor,\n",
      "                           steps: int) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    Create interpolation between two attribute configurations.\n",
      "\n",
      "    Args:\n",
      "        start_attrs: Starting attribute tensor\n",
      "        end_attrs: Ending attribute tensor\n",
      "        steps: Number of interpolation steps\n",
      "\n",
      "    Returns:\n",
      "        torch.Tensor: Interpolated attribute tensors\n",
      "    \"\"\"\n",
      "    # For categorical attributes, we'll gradually change probability distribution\n",
      "    alphas = torch.linspace(0, 1, steps)\n",
      "    interp_attrs = []\n",
      "\n",
      "    for alpha in alphas:\n",
      "        # Use alpha to interpolate between one-hot encodings\n",
      "        interp = torch.lerp(start_attrs.float(), end_attrs.float(), alpha)\n",
      "        # Convert back to categorical by taking argmax\n",
      "        categorical = torch.argmax(interp, dim=-1)\n",
      "        interp_attrs.append(categorical)\n",
      "\n",
      "    return torch.stack(interp_attrs)\n",
      "\n",
      "\n",
      "def get_attribute_changes(orig_attrs: torch.Tensor,\n",
      "                          perturbed_attrs: torch.Tensor,\n",
      "                          attribute_names: List[str],\n",
      "                          attribute_values: Dict[str, List[str]]) -> List[Dict[str, str]]:\n",
      "    \"\"\"\n",
      "    Get human-readable description of attribute changes.\n",
      "\n",
      "    Args:\n",
      "        orig_attrs: Original attribute tensor\n",
      "        perturbed_attrs: Modified attribute tensor\n",
      "        attribute_names: List of attribute names\n",
      "        attribute_values: Dictionary mapping attributes to their possible values\n",
      "\n",
      "    Returns:\n",
      "        List[Dict[str, str]]: List of changes in format {\"attribute\": \"old -> new\"}\n",
      "    \"\"\"\n",
      "    changes = []\n",
      "\n",
      "    for i in range(orig_attrs.size(0)):\n",
      "        sample_changes = {}\n",
      "        for j, attr_name in enumerate(attribute_names):\n",
      "            orig_val = orig_attrs[i, j].item()\n",
      "            pert_val = perturbed_attrs[i, j].item()\n",
      "\n",
      "            if orig_val != pert_val:\n",
      "                values = attribute_values[attr_name]\n",
      "                change = f\"{values[orig_val]} → {values[pert_val]}\"\n",
      "                sample_changes[attr_name] = change\n",
      "\n",
      "        changes.append(sample_changes)\n",
      "\n",
      "    return changes\n",
      "\n",
      "\n",
      "def attribute_to_text(attrs: torch.Tensor,\n",
      "                      attribute_names: List[str],\n",
      "                      attribute_values: Dict[str, List[str]]) -> List[str]:\n",
      "    \"\"\"\n",
      "    Convert attribute tensor to human-readable text descriptions.\n",
      "\n",
      "    Args:\n",
      "        attrs: Attribute tensor\n",
      "        attribute_names: List of attribute names\n",
      "        attribute_values: Dictionary mapping attributes to their possible values\n",
      "\n",
      "    Returns:\n",
      "        List[str]: Human-readable descriptions\n",
      "    \"\"\"\n",
      "    descriptions = []\n",
      "\n",
      "    for i in range(attrs.size(0)):\n",
      "        attr_strs = []\n",
      "        for j, attr_name in enumerate(attribute_names):\n",
      "            val_idx = attrs[i, j].item()\n",
      "            value = attribute_values[attr_name][val_idx]\n",
      "            attr_strs.append(f\"{attr_name}: {value}\")\n",
      "        descriptions.append(\" | \".join(attr_strs))\n",
      "\n",
      "    return descriptions\n",
      "\n",
      "\n",
      "def validate_attributes_tensor(attrs: torch.Tensor,\n",
      "                               attribute_dims: Dict[str, int]) -> bool:\n",
      "    \"\"\"\n",
      "    Validate attribute tensor against expected dimensions.\n",
      "\n",
      "    Args:\n",
      "        attrs: Attribute tensor to validate\n",
      "        attribute_dims: Dictionary of expected attribute dimensions\n",
      "\n",
      "    Returns:\n",
      "        bool: True if tensor is valid\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If tensor is invalid\n",
      "    \"\"\"\n",
      "    expected_dims = len(attribute_dims)\n",
      "    if attrs.dim() != 2:\n",
      "        raise ValueError(f\"Expected 2D tensor, got {attrs.dim()}D\")\n",
      "\n",
      "    if attrs.size(1) != expected_dims:\n",
      "        raise ValueError(f\"Expected {expected_dims} attributes, got {attrs.size(1)}\")\n",
      "\n",
      "    for i, (attr_name, num_values) in enumerate(attribute_dims.items()):\n",
      "        if torch.any((attrs[:, i] < 0) | (attrs[:, i] >= num_values)):\n",
      "            raise ValueError(f\"Invalid values for attribute {attr_name}\")\n",
      "\n",
      "    return True\n",
      "----------------------------------------\n",
      "File: clip_loss.py\n",
      "\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "import clip\n",
      "from typing import Dict, List, Union\n",
      "import random\n",
      "\n",
      "\n",
      "class CLIPAttributeConsistency:\n",
      "    def __init__(self, clip_model_name: str = \"ViT-B/32\", device: str = \"cuda\"):\n",
      "        \"\"\"\n",
      "        Initialize CLIP-based attribute consistency checker.\n",
      "\n",
      "        Args:\n",
      "            clip_model_name: Name of the CLIP model to use\n",
      "            device: Device to run the model on\n",
      "        \"\"\"\n",
      "        self.device = device\n",
      "        self.model, self.preprocess = clip.load(clip_model_name, device=device)\n",
      "        self.attribute_templates = {}\n",
      "\n",
      "    def register_templates(self, templates: Dict[str, Dict[int, List[str]]]):\n",
      "        \"\"\"\n",
      "        Register templates for attributes.\n",
      "\n",
      "        Args:\n",
      "            templates: Dictionary of attribute templates in the format:\n",
      "                {\n",
      "                    'attribute_name': {\n",
      "                        value_index: ['template1', 'template2', ...],\n",
      "                        ...\n",
      "                    },\n",
      "                    ...\n",
      "                }\n",
      "        \"\"\"\n",
      "        self.attribute_templates = templates\n",
      "\n",
      "    def generate_default_templates(self, attribute_info: Dict[str, Dict]):\n",
      "        \"\"\"\n",
      "        Generate default templates for attributes if none are provided.\n",
      "\n",
      "        Args:\n",
      "            attribute_info: Dictionary containing attribute information:\n",
      "                {\n",
      "                    'attribute_name': {\n",
      "                        'num_values': int,\n",
      "                        'values': List[str/int]\n",
      "                    },\n",
      "                    ...\n",
      "                }\n",
      "        \"\"\"\n",
      "        templates = {}\n",
      "\n",
      "        for attr_name, info in attribute_info.items():\n",
      "            templates[attr_name] = {}\n",
      "\n",
      "            for idx, value in enumerate(info['values']):\n",
      "                # Convert value to string if it's not already\n",
      "                value_str = str(value).lower().replace('_', ' ')\n",
      "\n",
      "                # Generate multiple templates for each value\n",
      "                templates[attr_name][idx] = [\n",
      "                    f\"a photo of a person with {value_str} features\",\n",
      "                    f\"a {value_str} person\",\n",
      "                    f\"a face showing {value_str} characteristics\",\n",
      "                    f\"a portrait of someone with {value_str} appearance\",\n",
      "                    f\"a photograph depicting {value_str} traits\"\n",
      "                ]\n",
      "\n",
      "        self.attribute_templates = templates\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def get_image_features(self, images: torch.Tensor) -> torch.Tensor:\n",
      "        \"\"\"Get normalized CLIP image features.\"\"\"\n",
      "        # Resize images to CLIP input size\n",
      "        images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
      "        image_features = self.model.encode_image(images)\n",
      "        return image_features / image_features.norm(dim=-1, keepdim=True)\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def get_text_features(self, descriptions: List[str]) -> torch.Tensor:\n",
      "        \"\"\"Get normalized CLIP text features.\"\"\"\n",
      "        text_tokens = clip.tokenize(descriptions).to(self.device)\n",
      "        text_features = self.model.encode_text(text_tokens)\n",
      "        return text_features / text_features.norm(dim=-1, keepdim=True)\n",
      "\n",
      "    def compute_attribute_loss(self, images: torch.Tensor, attrs: torch.Tensor,\n",
      "                               attribute_names: List[str]) -> torch.Tensor:\n",
      "        \"\"\"\n",
      "        Compute CLIP-based attribute consistency loss for a batch.\n",
      "\n",
      "        Args:\n",
      "            images: Batch of images\n",
      "            attrs: Tensor of attribute values\n",
      "            attribute_names: List of attribute names in order\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: Computed loss\n",
      "        \"\"\"\n",
      "        if not self.attribute_templates:\n",
      "            raise ValueError(\"No templates registered. Call register_templates first.\")\n",
      "\n",
      "        batch_size = images.size(0)\n",
      "        total_loss = torch.tensor(0.0, device=self.device)\n",
      "        num_attributes = len(attribute_names)\n",
      "\n",
      "        # Get image features once\n",
      "        image_features = self.get_image_features(images)\n",
      "\n",
      "        # For each image in the batch, randomly select an attribute to verify\n",
      "        for i in range(batch_size):\n",
      "            # Randomly select an attribute to verify\n",
      "            attr_idx = random.randint(0, num_attributes - 1)\n",
      "            attr_name = attribute_names[attr_idx]\n",
      "\n",
      "            # Get the actual value for this attribute\n",
      "            current_value = attrs[i, attr_idx].item()\n",
      "\n",
      "            # Get templates for the current value\n",
      "            if current_value not in self.attribute_templates[attr_name]:\n",
      "                continue\n",
      "\n",
      "            current_templates = self.attribute_templates[attr_name][current_value]\n",
      "\n",
      "            # Select a different value for contrast\n",
      "            possible_values = list(self.attribute_templates[attr_name].keys())\n",
      "            possible_values.remove(current_value)\n",
      "            contrast_value = random.choice(possible_values)\n",
      "            contrast_templates = self.attribute_templates[attr_name][contrast_value]\n",
      "\n",
      "            # Get text features for both current and contrast templates\n",
      "            current_text_features = self.get_text_features(current_templates)\n",
      "            contrast_text_features = self.get_text_features(contrast_templates)\n",
      "\n",
      "            # Compute similarities\n",
      "            current_sim = (image_features[i:i + 1] @ current_text_features.T).mean()\n",
      "            contrast_sim = (image_features[i:i + 1] @ contrast_text_features.T).mean()\n",
      "\n",
      "            # Compute contrastive loss with margin\n",
      "            margin = 0.2\n",
      "            loss = torch.relu(contrast_sim - current_sim + margin)\n",
      "            total_loss += loss\n",
      "\n",
      "        # Normalize by batch size\n",
      "        return total_loss / batch_size\n",
      "\n",
      "    def verify_single_attribute(self, image: torch.Tensor, attr_name: str,\n",
      "                                attr_value: int) -> float:\n",
      "        \"\"\"\n",
      "        Verify a single attribute value for an image.\n",
      "\n",
      "        Args:\n",
      "            image: Single image tensor\n",
      "            attr_name: Name of the attribute to verify\n",
      "            attr_value: Value of the attribute to verify\n",
      "\n",
      "        Returns:\n",
      "            float: Confidence score for the attribute (0 to 1)\n",
      "        \"\"\"\n",
      "        with torch.no_grad():\n",
      "            # Get image features\n",
      "            image_features = self.get_image_features(image.unsqueeze(0))\n",
      "\n",
      "            # Get templates for this attribute value\n",
      "            templates = self.attribute_templates[attr_name][attr_value]\n",
      "\n",
      "            # Get text features\n",
      "            text_features = self.get_text_features(templates)\n",
      "\n",
      "            # Compute similarity\n",
      "            similarity = (image_features @ text_features.T).mean().item()\n",
      "\n",
      "            # Convert to probability\n",
      "            return torch.sigmoid(torch.tensor(similarity * 100.0)).item()\n",
      "\n",
      "    def batch_verify_attributes(self, images: torch.Tensor,\n",
      "                                attrs: torch.Tensor,\n",
      "                                attribute_names: List[str]) -> torch.Tensor:\n",
      "        \"\"\"\n",
      "        Verify all attributes for a batch of images.\n",
      "\n",
      "        Args:\n",
      "            images: Batch of images\n",
      "            attrs: Tensor of attribute values\n",
      "            attribute_names: List of attribute names in order\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor: Tensor of confidence scores for each attribute\n",
      "        \"\"\"\n",
      "        batch_size = images.size(0)\n",
      "        num_attributes = len(attribute_names)\n",
      "        confidences = torch.zeros(batch_size, num_attributes, device=self.device)\n",
      "\n",
      "        with torch.no_grad():\n",
      "            image_features = self.get_image_features(images)\n",
      "\n",
      "            for attr_idx, attr_name in enumerate(attribute_names):\n",
      "                for i in range(batch_size):\n",
      "                    attr_value = attrs[i, attr_idx].item()\n",
      "                    templates = self.attribute_templates[attr_name][attr_value]\n",
      "                    text_features = self.get_text_features(templates)\n",
      "\n",
      "                    similarity = (image_features[i:i + 1] @ text_features.T).mean()\n",
      "                    confidences[i, attr_idx] = torch.sigmoid(similarity * 100.0)\n",
      "\n",
      "        return confidences\n",
      "----------------------------------------\n",
      "File: celeba_project.py\n",
      "\n",
      "import os\n",
      "import tempfile\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "from torch.utils.data import DataLoader\n",
      "from tqdm import tqdm\n",
      "import matplotlib.pyplot as plt\n",
      "import torchvision.transforms as transforms\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "from models import CVAE, loss_function\n",
      "from dataset import CelebaDataset\n",
      "from clip_classifier import CustomAttributeClassifier\n",
      "from clip_loss import CLIPAttributeConsistency\n",
      "from utils import (validate_attribute_config,\n",
      "                   convert_clip_to_vae_format,\n",
      "                   create_attribute_noise)\n",
      "from plots import (plot_reconstructions_with_perturbations,\n",
      "                   plot_training_progress,\n",
      "                   plot_attribute_distributions)\n",
      "\n",
      "# Define transform for images\n",
      "transform = transforms.Compose([\n",
      "    transforms.Resize((64, 64)),\n",
      "    transforms.ToTensor(),\n",
      "])\n",
      "\n",
      "\n",
      "def generate_clip_labels(user_attributes, img_dir, max_images=100):\n",
      "    \"\"\"\n",
      "    Generate labels for images using CLIP classifier.\n",
      "\n",
      "    Args:\n",
      "        user_attributes: Dictionary of attribute names and their possible values\n",
      "        img_dir: Directory containing the images\n",
      "        max_images: Maximum number of images to process\n",
      "\n",
      "    Returns:\n",
      "        results_df: DataFrame containing image IDs and their attribute classifications\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If no valid images are found or processing fails\n",
      "    \"\"\"\n",
      "    print(\"Generating labels using CLIP...\")\n",
      "\n",
      "    # Load environment variables\n",
      "    load_dotenv()\n",
      "\n",
      "    # Get the API key\n",
      "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "    # Create the classifier with explicit API key\n",
      "    classifier = CustomAttributeClassifier(\n",
      "        clip_model_name=\"ViT-B/16\",\n",
      "        openai_api_key=api_key\n",
      "    )\n",
      "\n",
      "    # Validate directory exists\n",
      "    if not os.path.exists(img_dir):\n",
      "        raise ValueError(f\"Image directory not found: {img_dir}\")\n",
      "\n",
      "    # Get list of all image files\n",
      "    image_files = [f for f in os.listdir(img_dir)\n",
      "                   if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
      "\n",
      "    if not image_files:\n",
      "        raise ValueError(f\"No valid image files found in {img_dir}\")\n",
      "\n",
      "    # Take only first max_images\n",
      "    selected_files = image_files[:max_images]\n",
      "    print(f\"\\nProcessing {len(selected_files)} images out of {len(image_files)} total images\")\n",
      "\n",
      "    # Create a temporary directory with symlinks to selected images\n",
      "    with tempfile.TemporaryDirectory() as temp_dir:\n",
      "        # Create symlinks for selected images\n",
      "        for img_file in selected_files:\n",
      "            src = os.path.join(img_dir, img_file)\n",
      "            dst = os.path.join(temp_dir, img_file)\n",
      "            if os.path.exists(src):\n",
      "                os.symlink(src, dst)\n",
      "\n",
      "        # Classify only selected images\n",
      "        results_df = classifier.classify_images(\n",
      "            img_dir=temp_dir,\n",
      "            attribute_values=user_attributes,\n",
      "            batch_size=16\n",
      "        )\n",
      "\n",
      "    # Validate results\n",
      "    if results_df is None or len(results_df) == 0:\n",
      "        raise ValueError(\n",
      "            \"CLIP classification failed to produce any valid results. Please check your input data and paths.\")\n",
      "\n",
      "    return results_df\n",
      "\n",
      "\n",
      "def train_vae_model(results_df, user_attributes, img_dir, device=\"cuda\",\n",
      "                    batch_size=32, num_epochs=100, vis_dir=\"training_outputs\",\n",
      "                    resampling_strategy='oversample'):\n",
      "    \"\"\"\n",
      "    Train the VAE model using resampling strategies for handling imbalanced data.\n",
      "\n",
      "    Args:\n",
      "        results_df: DataFrame with CLIP-generated labels\n",
      "        user_attributes: Dictionary of attribute names and their possible values\n",
      "        img_dir: Directory containing images\n",
      "        device: Device to run training on\n",
      "        batch_size: Training batch size\n",
      "        num_epochs: Number of training epochs\n",
      "        vis_dir: Directory to save visualizations\n",
      "        resampling_strategy: One of ['none', 'oversample', 'undersample', 'smote']\n",
      "    \"\"\"\n",
      "    print(\"\\nPreparing for training...\")\n",
      "\n",
      "    # Convert CLIP results to VAE format\n",
      "    vae_df, attribute_dims = convert_clip_to_vae_format(\n",
      "        results_df,\n",
      "        list(user_attributes.keys())\n",
      "    )\n",
      "\n",
      "    # Print initial class distribution\n",
      "    print(\"\\nInitial class distribution:\")\n",
      "    for attr in user_attributes.keys():\n",
      "        print(f\"\\n{attr} distribution:\")\n",
      "        counts = vae_df[attr].value_counts().sort_index()\n",
      "        total = len(vae_df)\n",
      "        for idx, count in counts.items():\n",
      "            value = user_attributes[attr][idx]\n",
      "            percentage = (count / total) * 100\n",
      "            print(f\"{value:<20}: {count:>6,} ({percentage:>6.1f}%)\")\n",
      "\n",
      "    # Initialize dataset with resampling\n",
      "    dataset = CelebaDataset(\n",
      "        df=vae_df,\n",
      "        img_dir=img_dir,\n",
      "        transform=transform,\n",
      "        attribute_names=list(user_attributes.keys()),\n",
      "        resampling_strategy=resampling_strategy\n",
      "    )\n",
      "\n",
      "    # Print resampled class distribution\n",
      "    print(f\"\\nClass distribution after {resampling_strategy}:\")\n",
      "    for attr in user_attributes.keys():\n",
      "        print(f\"\\n{attr} distribution:\")\n",
      "        counts = dataset.df[attr].value_counts().sort_index()\n",
      "        total = len(dataset.df)\n",
      "        for idx, count in counts.items():\n",
      "            value = user_attributes[attr][idx]\n",
      "            percentage = (count / total) * 100\n",
      "            print(f\"{value:<20}: {count:>6,} ({percentage:>6.1f}%)\")\n",
      "\n",
      "    dataloader = DataLoader(\n",
      "        dataset,\n",
      "        batch_size=batch_size,\n",
      "        shuffle=True,\n",
      "        num_workers=4,\n",
      "        pin_memory=True\n",
      "    )\n",
      "\n",
      "    # Initialize models\n",
      "    vae = CVAE(attribute_dims=attribute_dims).to(device)\n",
      "    clip_consistency = CLIPAttributeConsistency(device=device)\n",
      "    clip_consistency.generate_default_templates(dataset.get_attribute_info())\n",
      "    optimizer = optim.Adam(vae.parameters(), lr=0.0001)\n",
      "\n",
      "    # Initialize loss history\n",
      "    loss_history = {\n",
      "        'total': [],\n",
      "        'reconstruction': [],\n",
      "        'kl': [],\n",
      "        'clip': []\n",
      "    }\n",
      "\n",
      "    print(\"\\nStarting training...\")\n",
      "    os.makedirs(vis_dir, exist_ok=True)\n",
      "\n",
      "    # Training loop\n",
      "    for epoch in range(num_epochs):\n",
      "        vae.train()\n",
      "        epoch_losses = {k: 0.0 for k in loss_history.keys()}\n",
      "\n",
      "        pbar = tqdm(total=len(dataloader),\n",
      "                    desc=f'Epoch {epoch}/{num_epochs}',\n",
      "                    position=0,\n",
      "                    leave=True,\n",
      "                    dynamic_ncols=True)\n",
      "\n",
      "        # Store first batch for visualization\n",
      "        vis_batch = None\n",
      "\n",
      "        for batch_idx, (images, attrs) in enumerate(dataloader):\n",
      "            if batch_idx == 0:\n",
      "                vis_batch = (images[:5].clone(), attrs[:5].clone())\n",
      "\n",
      "            images = images.to(device)\n",
      "            attrs = attrs.to(device)\n",
      "\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            # Forward pass\n",
      "            recon_images, mu, logvar = vae(images, attrs)\n",
      "\n",
      "            # Compute losses\n",
      "            total_loss, recon_loss, kl_loss, clip_loss = loss_function(\n",
      "                recon_images, images, mu, logvar,\n",
      "                clip_consistency=clip_consistency,\n",
      "                attrs=attrs,\n",
      "                attribute_names=list(user_attributes.keys()),\n",
      "                beta_vae=1.0,\n",
      "                beta_clip=1\n",
      "            )\n",
      "\n",
      "            # Backward pass\n",
      "            total_loss.backward()\n",
      "            optimizer.step()\n",
      "\n",
      "            # Update loss tracking\n",
      "            epoch_losses['total'] += total_loss.item()\n",
      "            epoch_losses['reconstruction'] += recon_loss.item()\n",
      "            epoch_losses['kl'] += kl_loss.item()\n",
      "            epoch_losses['clip'] += clip_loss.item()\n",
      "\n",
      "            pbar.update(1)\n",
      "\n",
      "        pbar.close()\n",
      "\n",
      "        # Average losses\n",
      "        for k in epoch_losses:\n",
      "            avg_loss = epoch_losses[k] / len(dataloader)\n",
      "            loss_history[k].append(avg_loss)\n",
      "\n",
      "        # Print progress on same line\n",
      "        loss_str = f\"Epoch {epoch}/{num_epochs} | \" + \" | \".join(\n",
      "            [f\"{k.capitalize()}: {v / len(dataloader):.6f}\" for k, v in epoch_losses.items()]\n",
      "        )\n",
      "        tqdm.write(loss_str)\n",
      "\n",
      "        # Save visualizations and checkpoint every 10 epochs\n",
      "        if epoch % 1 == 0:\n",
      "            if vis_batch is not None:\n",
      "                vae.eval()\n",
      "                with torch.no_grad():\n",
      "                    images, attrs = vis_batch\n",
      "                    images = images.to(device)\n",
      "                    attrs = attrs.to(device)\n",
      "\n",
      "                    # Get reconstructions\n",
      "                    recon_images, mu, logvar = vae(images, attrs)\n",
      "\n",
      "                    # Create perturbed versions\n",
      "                    perturbed_attrs = attrs.clone()\n",
      "                    for i in range(len(perturbed_attrs)):\n",
      "                        attr_idx = torch.randint(0, len(user_attributes), (1,)).item()\n",
      "                        current_val = perturbed_attrs[i, attr_idx].item()\n",
      "                        num_values = len(user_attributes[list(user_attributes.keys())[attr_idx]])\n",
      "                        new_val = (current_val + torch.randint(1, num_values, (1,)).item()) % num_values\n",
      "                        perturbed_attrs[i, attr_idx] = new_val\n",
      "\n",
      "                    # Generate perturbed images\n",
      "                    perturbed_z = torch.cat([mu, perturbed_attrs], dim=1)\n",
      "                    perturbed_images = vae.decoder(perturbed_z)\n",
      "\n",
      "                    # Plot results\n",
      "                    plot_reconstructions_with_perturbations(\n",
      "                        images, recon_images, perturbed_images,\n",
      "                        attrs, perturbed_attrs,\n",
      "                        list(user_attributes.keys()),\n",
      "                        user_attributes,\n",
      "                        epoch,\n",
      "                        vis_dir\n",
      "                    )\n",
      "\n",
      "            # Save checkpoint\n",
      "            checkpoint = {\n",
      "                'epoch': epoch,\n",
      "                'model_state_dict': vae.state_dict(),\n",
      "                'optimizer_state_dict': optimizer.state_dict(),\n",
      "                'loss': loss_history,\n",
      "                'attribute_dims': attribute_dims\n",
      "            }\n",
      "            torch.save(checkpoint, os.path.join(vis_dir, f'checkpoint_epoch_{epoch}.pt'))\n",
      "\n",
      "            # Save training progress plot\n",
      "            plot_training_progress(loss_history, vis_dir)\n",
      "\n",
      "    return vae, loss_history\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "File: running.py\n",
      "\n",
      "import os\n",
      "\n",
      "import pandas as pd\n",
      "import torch\n",
      "\n",
      "from celeba_project import train_vae_model\n",
      "from utils import validate_attribute_config\n",
      "\n",
      "# User-defined attributes\n",
      "user_attributes = {\n",
      "    \"ethnicity\": [\"African\", \"Asian\", \"European\"],\n",
      "    \"facial_expression\": [\"happy\", \"sad\", \"angry\"]\n",
      "}\n",
      "results_df = pd.read_csv(\"results.csv\")\n",
      "\n",
      "validate_attribute_config({name: len(values) for name, values in user_attributes.items()})\n",
      "\n",
      "# Setup directories\n",
      "img_dir = r\"/home/omrid/Desktop/jungo /projectCLIPvae/celeba_dataset/img_align_celeba/img_align_celeba\"\n",
      "vis_dir = os.path.join('training_outputs', 'visualizations')\n",
      "os.makedirs(vis_dir, exist_ok=True)\n",
      "\n",
      "# Train VAE\n",
      "print(\"\\nStep 2: Training VAE...\")\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "vae, loss_history = train_vae_model(\n",
      "    results_df=results_df,\n",
      "    user_attributes=user_attributes,\n",
      "    img_dir=img_dir,\n",
      "    device=device,\n",
      "    batch_size=32,\n",
      "    num_epochs=100,\n",
      "    vis_dir=vis_dir\n",
      ")\n",
      "\n",
      "print(\"\\nTraining complete! Check the visualization directory for results.\")\n",
      "----------------------------------------\n",
      "File: dataset.py\n",
      "\n",
      "import os\n",
      "import torch\n",
      "from PIL import Image\n",
      "from torch.utils.data import Dataset\n",
      "import torchvision.transforms as transforms\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class CelebaDataset(Dataset):\n",
      "    \"\"\"\n",
      "    A flexible dataset class with resampling strategies for imbalanced attributes.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, df, img_dir, transform=None, attribute_names=None,\n",
      "                 resampling_strategy='none', sampling_attributes=None):\n",
      "        \"\"\"\n",
      "        Initialize dataset with resampling options.\n",
      "\n",
      "        Args:\n",
      "            df (pd.DataFrame): DataFrame containing 'image_id' and attribute columns\n",
      "            img_dir (str): Directory containing images\n",
      "            transform (callable, optional): Optional transform for images\n",
      "            attribute_names (list, optional): List of attribute columns to use\n",
      "            resampling_strategy (str): One of ['none', 'oversample', 'undersample', 'smote']\n",
      "            sampling_attributes (list, optional): Specific attributes to balance. If None, balance all\n",
      "        \"\"\"\n",
      "        self.df = df.copy()\n",
      "        self.img_dir = img_dir\n",
      "        self.transform = transform or transforms.Compose([\n",
      "            transforms.Resize((64, 64)),\n",
      "            transforms.ToTensor(),\n",
      "        ])\n",
      "\n",
      "        # Set attribute names\n",
      "        self.attribute_names = attribute_names or [col for col in df.columns if col != 'image_id']\n",
      "\n",
      "        # Validate attributes exist\n",
      "        missing_attrs = [attr for attr in self.attribute_names if attr not in df.columns]\n",
      "        if missing_attrs:\n",
      "            raise ValueError(f\"Attributes {missing_attrs} not found in DataFrame\")\n",
      "\n",
      "        # Create attribute information dictionary\n",
      "        self.attribute_info = {}\n",
      "        for attr in self.attribute_names:\n",
      "            unique_values = sorted(df[attr].unique())\n",
      "            self.attribute_info[attr] = {\n",
      "                'num_values': len(unique_values),\n",
      "                'values': unique_values\n",
      "            }\n",
      "\n",
      "        # Apply resampling if requested\n",
      "        self.sampling_attributes = sampling_attributes or self.attribute_names\n",
      "        if resampling_strategy != 'none':\n",
      "            self._apply_resampling(resampling_strategy)\n",
      "\n",
      "        # Store synthetic features if SMOTE is used\n",
      "        self.synthetic_features = None\n",
      "\n",
      "        # Validate image files exist\n",
      "        self._validate_images()\n",
      "\n",
      "    def _validate_images(self):\n",
      "        \"\"\"Validate that all images in the DataFrame exist in the directory.\"\"\"\n",
      "        missing_images = []\n",
      "        for img_name in self.df['image_id']:\n",
      "            # Skip validation for synthetic images\n",
      "            if str(img_name).startswith('synthetic_'):\n",
      "                continue\n",
      "            img_path = os.path.join(self.img_dir, img_name)\n",
      "            if not os.path.exists(img_path):\n",
      "                missing_images.append(img_name)\n",
      "\n",
      "        if missing_images:\n",
      "            raise FileNotFoundError(\n",
      "                f\"Could not find {len(missing_images)} images. \"\n",
      "                f\"First few missing: {missing_images[:5]}\"\n",
      "            )\n",
      "\n",
      "    def _apply_resampling(self, strategy):\n",
      "        \"\"\"\n",
      "        Apply the specified resampling strategy.\n",
      "        \"\"\"\n",
      "        print(f\"\\nApplying {strategy} resampling strategy...\")\n",
      "\n",
      "        if strategy == 'oversample':\n",
      "            self._apply_oversampling()\n",
      "        elif strategy == 'undersample':\n",
      "            self._apply_undersampling()\n",
      "        elif strategy == 'smote':\n",
      "            self._apply_smote()\n",
      "        else:\n",
      "            raise ValueError(f\"Unknown resampling strategy: {strategy}\")\n",
      "\n",
      "    def _apply_oversampling(self):\n",
      "        \"\"\"\n",
      "        Oversample minority classes to match majority class.\n",
      "        \"\"\"\n",
      "        resampled_dfs = []\n",
      "\n",
      "        # Create combination groups for selected attributes\n",
      "        self.df['_group'] = self.df[self.sampling_attributes].apply(tuple, axis=1)\n",
      "        group_counts = self.df['_group'].value_counts()\n",
      "        max_count = group_counts.max()\n",
      "\n",
      "        print(f\"Oversampling to {max_count} samples per group...\")\n",
      "\n",
      "        # Oversample each group\n",
      "        for group in group_counts.index:\n",
      "            group_df = self.df[self.df['_group'] == group]\n",
      "            if len(group_df) < max_count:\n",
      "                # Randomly sample with replacement to match max_count\n",
      "                resampled = group_df.sample(n=max_count, replace=True)\n",
      "                resampled_dfs.append(resampled)\n",
      "            else:\n",
      "                resampled_dfs.append(group_df)\n",
      "\n",
      "        self.df = pd.concat(resampled_dfs, ignore_index=True)\n",
      "        self.df = self.df.drop('_group', axis=1)\n",
      "\n",
      "    def _apply_undersampling(self):\n",
      "        \"\"\"\n",
      "        Undersample majority classes to match minority class.\n",
      "        \"\"\"\n",
      "        resampled_dfs = []\n",
      "\n",
      "        # Create combination groups for selected attributes\n",
      "        self.df['_group'] = self.df[self.sampling_attributes].apply(tuple, axis=1)\n",
      "        group_counts = self.df['_group'].value_counts()\n",
      "        min_count = group_counts.min()\n",
      "\n",
      "        print(f\"Undersampling to {min_count} samples per group...\")\n",
      "\n",
      "        # Undersample each group\n",
      "        for group in group_counts.index:\n",
      "            group_df = self.df[self.df['_group'] == group]\n",
      "            if len(group_df) > min_count:\n",
      "                # Randomly sample without replacement to match min_count\n",
      "                resampled = group_df.sample(n=min_count, replace=False)\n",
      "                resampled_dfs.append(resampled)\n",
      "            else:\n",
      "                resampled_dfs.append(group_df)\n",
      "\n",
      "        self.df = pd.concat(resampled_dfs, ignore_index=True)\n",
      "        self.df = self.df.drop('_group', axis=1)\n",
      "\n",
      "    def _apply_smote(self):\n",
      "        \"\"\"\n",
      "        Apply SMOTE (Synthetic Minority Over-sampling Technique).\n",
      "        \"\"\"\n",
      "        try:\n",
      "            from imblearn.over_sampling import SMOTE\n",
      "        except ImportError:\n",
      "            raise ImportError(\"Please install imbalanced-learn: pip install imbalanced-learn\")\n",
      "\n",
      "        print(\"Applying SMOTE resampling...\")\n",
      "\n",
      "        # Extract features for SMOTE\n",
      "        features = []\n",
      "        for _, row in self.df.iterrows():\n",
      "            img_path = os.path.join(self.img_dir, row['image_id'])\n",
      "            img = Image.open(img_path).convert('RGB')\n",
      "            img = self.transform(img)\n",
      "            features.append(img.flatten().numpy())\n",
      "\n",
      "        features = np.stack(features)\n",
      "\n",
      "        # Create combined target for selected attributes\n",
      "        self.df['_group'] = self.df[self.sampling_attributes].apply(tuple, axis=1)\n",
      "        group_mapping = {group: idx for idx, group in enumerate(self.df['_group'].unique())}\n",
      "        targets = self.df['_group'].map(group_mapping)\n",
      "\n",
      "        # Apply SMOTE\n",
      "        smote = SMOTE(random_state=42)\n",
      "        features_resampled, targets_resampled = smote.fit_resample(features, targets)\n",
      "\n",
      "        # Store synthetic features\n",
      "        self.synthetic_features = features_resampled[len(self.df):]\n",
      "\n",
      "        # Reconstruct DataFrame\n",
      "        new_rows = []\n",
      "        for idx, target in enumerate(targets_resampled):\n",
      "            if idx < len(self.df):\n",
      "                new_rows.append(self.df.iloc[idx])\n",
      "            else:\n",
      "                original_group = {v: k for k, v in group_mapping.items()}[target]\n",
      "                new_row = {'image_id': f'synthetic_{idx}'}\n",
      "                for attr_idx, attr in enumerate(self.sampling_attributes):\n",
      "                    new_row[attr] = original_group[attr_idx]\n",
      "                new_rows.append(new_row)\n",
      "\n",
      "        self.df = pd.DataFrame(new_rows)\n",
      "        if '_group' in self.df.columns:\n",
      "            self.df = self.df.drop('_group', axis=1)\n",
      "\n",
      "    def get_attribute_dims(self):\n",
      "        \"\"\"Get dictionary of attribute dimensions.\"\"\"\n",
      "        return {attr: info['num_values'] for attr, info in self.attribute_info.items()}\n",
      "\n",
      "    def get_attribute_info(self):\n",
      "        \"\"\"Get detailed attribute information.\"\"\"\n",
      "        return self.attribute_info.copy()\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.df)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        \"\"\"Get a single item from the dataset.\"\"\"\n",
      "        img_name = self.df.iloc[idx]['image_id']\n",
      "\n",
      "        # Handle synthetic images from SMOTE\n",
      "        if str(img_name).startswith('synthetic_'):\n",
      "            synthetic_idx = int(img_name.split('_')[1]) - len(self.df) + len(self.synthetic_features)\n",
      "            image = torch.from_numpy(self.synthetic_features[synthetic_idx]).view(3, 64, 64)\n",
      "        else:\n",
      "            # Load real image\n",
      "            img_path = os.path.join(self.img_dir, img_name)\n",
      "            try:\n",
      "                image = Image.open(img_path).convert('RGB')\n",
      "                image = self.transform(image)\n",
      "            except Exception as e:\n",
      "                print(f\"Error loading image {img_path}: {str(e)}\")\n",
      "                raise\n",
      "\n",
      "        # Extract attributes\n",
      "        attrs = []\n",
      "        for attr_name in self.attribute_names:\n",
      "            attr_value = self.df.iloc[idx][attr_name]\n",
      "            attrs.append(attr_value)\n",
      "\n",
      "        return image, torch.tensor(attrs, dtype=torch.long)\n",
      "\n",
      "\n",
      "def create_dataloader(df, img_dir, batch_size=32, num_workers=4, **dataset_kwargs):\n",
      "    \"\"\"\n",
      "    Create a DataLoader with the specified parameters.\n",
      "\n",
      "    Args:\n",
      "        df (pd.DataFrame): DataFrame containing image IDs and attributes\n",
      "        img_dir (str): Directory containing images\n",
      "        batch_size (int): Batch size for training\n",
      "        num_workers (int): Number of worker processes for data loading\n",
      "        **dataset_kwargs: Additional arguments to pass to CelebaDataset\n",
      "\n",
      "    Returns:\n",
      "        DataLoader: Configured DataLoader instance\n",
      "        dict: Attribute dimensions dictionary for VAE initialization\n",
      "    \"\"\"\n",
      "    # Create dataset\n",
      "    dataset = CelebaDataset(df=df, img_dir=img_dir, **dataset_kwargs)\n",
      "\n",
      "    # Create dataloader\n",
      "    dataloader = torch.utils.data.DataLoader(\n",
      "        dataset,\n",
      "        batch_size=batch_size,\n",
      "        shuffle=True,\n",
      "        num_workers=num_workers,\n",
      "        pin_memory=True  # Faster data transfer to GPU\n",
      "    )\n",
      "\n",
      "    # Get attribute dimensions for VAE initialization\n",
      "    attribute_dims = dataset.get_attribute_dims()\n",
      "\n",
      "    return dataloader, attribute_dims\n",
      "----------------------------------------\n",
      "File: clip_classifier.py\n",
      "\n",
      "import torch\n",
      "import clip\n",
      "from PIL import Image\n",
      "import pandas as pd\n",
      "from dotenv import load_dotenv\n",
      "from tqdm import tqdm\n",
      "import os\n",
      "from typing import Dict, List, Union\n",
      "import torch.nn.functional as F\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
      "import json\n",
      "import dotenv\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "\n",
      "class TemplateGenerator:\n",
      "    def __init__(self, api_key: str):\n",
      "        \"\"\"Initialize the template generator with OpenAI API key\"\"\"\n",
      "        print(f\"\\nInitializing Template Generator...\")\n",
      "        print(f\"API Key provided: {'Yes' if api_key else 'No'}\")\n",
      "\n",
      "        try:\n",
      "            self.llm = ChatOpenAI(\n",
      "                model_name=\"gpt-3.5-turbo-0125\",\n",
      "                temperature=0.7,\n",
      "                openai_api_key=api_key\n",
      "            )\n",
      "            print(\"Successfully initialized ChatOpenAI\")\n",
      "        except Exception as e:\n",
      "            print(f\"Error initializing ChatOpenAI: {str(e)}\")\n",
      "            raise\n",
      "\n",
      "        # Create a clean, single-line prompt template\n",
      "        template = (\n",
      "            \"Generate natural language templates that describe a person's appearance for CLIP image classification.\\n\\n\"\n",
      "            \"IMPORTANT INSTRUCTIONS:\\n\"\n",
      "            \"- Each template should be a simple, clear sentence focusing ONLY on the specific attribute value\\n\"\n",
      "            \"- DO NOT add any assumptions or extra information beyond the attribute value\\n\"\n",
      "            \"- DO NOT specify age ranges, specific details, or additional characteristics\\n\"\n",
      "            \"- DO NOT use subjective or interpretive descriptions\\n\"\n",
      "            \"- FOCUS on distinguishing between the categories within each attribute\\n\"\n",
      "            \"- Keep descriptions general and factual\\n\"\n",
      "            \"- MAKE SURE to use the exact attribute values provided, without modifications\\n\\n\"\n",
      "            \"For example, if given:\\n\"\n",
      "            \"ethnicity: African, Asian, European\\n\\n\"\n",
      "            \"Expected Output:\\n\"\n",
      "            '{{\\n'\n",
      "            '    \"ethnicity\": {{\\n'\n",
      "            '        \"African\": [\\n'\n",
      "            '            \"a photograph of a person with African features\",\\n'\n",
      "            '            \"an African person\",\\n'\n",
      "            '            \"a face with African characteristics\",\\n'\n",
      "            '            \"a portrait of an African person\",\\n'\n",
      "            '            \"a person of African descent\"\\n'\n",
      "            '        ],\\n'\n",
      "            '        \"Asian\": [\\n'\n",
      "            '            \"a photograph of a person with Asian features\",\\n'\n",
      "            '            \"an Asian person\",\\n'\n",
      "            '            \"a face with Asian characteristics\",\\n'\n",
      "            '            \"a portrait of an Asian person\",\\n'\n",
      "            '            \"a person of Asian descent\"\\n'\n",
      "            '        ],\\n'\n",
      "            '        \"European\": [\\n'\n",
      "            '            \"a photograph of a person with European features\",\\n'\n",
      "            '            \"a European person\",\\n'\n",
      "            '            \"a face with European characteristics\",\\n'\n",
      "            '            \"a portrait of a European person\",\\n'\n",
      "            '            \"a person of European descent\"\\n'\n",
      "            '        ]\\n'\n",
      "            '    }}\\n'\n",
      "            '}}\\n\\n'\n",
      "            \"Now, please generate templates for the following attributes and values:\\n\"\n",
      "            \"{attributes}\\n\\n\"\n",
      "            \"The output should be in the same JSON format as the example.\\n\"\n",
      "            \"Remember to keep descriptions simple and focused only on the specific attribute value.\\n\"\n",
      "            \"Only include the JSON output, no additional text.\"\n",
      "        )\n",
      "\n",
      "        self.prompt = PromptTemplate(\n",
      "            template=template,\n",
      "            input_variables=[\"attributes\"]\n",
      "        )\n",
      "\n",
      "    def generate_templates(self, attribute_values: Dict[str, List[str]]) -> Dict[str, Dict[str, List[str]]]:\n",
      "        \"\"\"Generate templates using GPT for given attributes and values\"\"\"\n",
      "        try:\n",
      "            # Format attributes for the prompt\n",
      "            attributes_str = \"\\n\".join(\n",
      "                f\"{attr}: {', '.join(values)}\"\n",
      "                for attr, values in attribute_values.items()\n",
      "            )\n",
      "\n",
      "            print(\"\\nGenerating templates for attributes:\")\n",
      "            print(attributes_str)\n",
      "\n",
      "            # Generate the prompt using the template\n",
      "            prompt = self.prompt.format(attributes=attributes_str)\n",
      "\n",
      "            print(\"\\nSending request to GPT...\")\n",
      "            # Get response from GPT\n",
      "            response = self.llm.predict(prompt)\n",
      "\n",
      "            print(\"\\nReceived response from GPT. First 200 characters:\")\n",
      "            print(response[:200] + \"...\" if len(response) > 200 else response)\n",
      "\n",
      "            # Parse the JSON response\n",
      "            try:\n",
      "                templates = json.loads(response)\n",
      "                print(\"\\nSuccessfully parsed JSON response\")\n",
      "                # Debug: Print template structure\n",
      "                print(\"\\nTemplate structure:\")\n",
      "                for attr, values in templates.items():\n",
      "                    print(f\"\\n{attr}:\")\n",
      "                    for val, temps in values.items():\n",
      "                        print(f\"  {val}: {len(temps)} templates\")\n",
      "                return templates\n",
      "            except json.JSONDecodeError as e:\n",
      "                print(f\"\\nError parsing GPT response as JSON: {str(e)}\")\n",
      "                print(\"Raw response:\", response)\n",
      "                return None\n",
      "\n",
      "        except Exception as e:\n",
      "            print(f\"\\nError in template generation: {str(e)}\")\n",
      "            import traceback\n",
      "            traceback.print_exc()\n",
      "            return None\n",
      "\n",
      "\n",
      "class CustomAttributeClassifier:\n",
      "    def __init__(self,\n",
      "                 clip_model_name: str = \"ViT-B/32\",\n",
      "                 device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
      "                 openai_api_key: str = None,\n",
      "                 debug: bool = False):\n",
      "        self.device = device\n",
      "        self.debug = debug\n",
      "        print(f\"Initializing CLIP model on {device}\")\n",
      "\n",
      "        # First try to get the API key from the parameter, then from environment\n",
      "        self.api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "        if self.api_key:\n",
      "            print(\"OpenAI API key found!\")\n",
      "            self.template_generator = TemplateGenerator(self.api_key)\n",
      "        else:\n",
      "            print(\"No OpenAI API key found in parameters or environment variables.\")\n",
      "            self.template_generator = None\n",
      "\n",
      "        self.model, self.preprocess = clip.load(clip_model_name, device=device)\n",
      "        self.templates = None\n",
      "        self.attribute_mappings = {}\n",
      "\n",
      "    def normalize_templates(self, gpt_templates: Dict) -> Dict:\n",
      "        \"\"\"\n",
      "        Normalize templates from GPT format to our indexed format\n",
      "        \"\"\"\n",
      "        normalized = {}\n",
      "        for attr, values in gpt_templates.items():\n",
      "            normalized[attr] = {}\n",
      "            # Create a mapping from value to index\n",
      "            value_to_idx = {value: idx for idx, value in self.attribute_mappings[attr].items()}\n",
      "\n",
      "            # Convert string keys to numeric indices\n",
      "            for value, templates in values.items():\n",
      "                if value in value_to_idx:\n",
      "                    idx = value_to_idx[value]\n",
      "                    normalized[attr][idx] = templates\n",
      "                else:\n",
      "                    print(f\"Warning: Value '{value}' not found in mappings for {attr}\")\n",
      "\n",
      "        return normalized\n",
      "\n",
      "    def print_templates(self):\n",
      "        \"\"\"Print the full template structure in a clear, organized format\"\"\"\n",
      "        print(\"\\nCurrent Templates:\")\n",
      "        print(\"=\" * 50)\n",
      "\n",
      "        for attr, templates in self.templates.items():\n",
      "            print(f\"\\n{attr.upper()}:\")\n",
      "            print(\"-\" * 30)\n",
      "\n",
      "            # Sort by index to maintain consistent order\n",
      "            for idx in sorted(templates.keys()):\n",
      "                value = self.attribute_mappings[attr][idx]\n",
      "                print(f\"\\n{value}:\")\n",
      "                for i, template in enumerate(templates[idx], 1):\n",
      "                    print(f\"  {i}. {template}\")\n",
      "\n",
      "        print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
      "\n",
      "    def create_text_templates(self, attribute_values: Dict[str, List[str]]) -> Dict:\n",
      "        \"\"\"Create text templates using GPT if available, otherwise use default templates\"\"\"\n",
      "        # First create attribute mappings\n",
      "        self.attribute_mappings = {\n",
      "            attr: {idx: value for idx, value in enumerate(values)}\n",
      "            for attr, values in attribute_values.items()\n",
      "        }\n",
      "\n",
      "        if self.template_generator:\n",
      "            print(\"\\nAttempting to generate templates using GPT...\")\n",
      "            try:\n",
      "                gpt_templates = self.template_generator.generate_templates(attribute_values)\n",
      "                if gpt_templates:\n",
      "                    print(\"Successfully generated templates using GPT!\")\n",
      "                    # Normalize the templates to use indices\n",
      "                    self.templates = self.normalize_templates(gpt_templates)\n",
      "                    return self.templates\n",
      "\n",
      "            except Exception as e:\n",
      "                print(f\"\\nError during GPT template generation: {str(e)}\")\n",
      "                print(\"Falling back to custom templates.\")\n",
      "        else:\n",
      "            print(\"\\nGPT template generator not available (no API key provided).\")\n",
      "            print(\"Using custom templates instead.\")\n",
      "\n",
      "        # Fallback to default templates\n",
      "        templates = {}\n",
      "        for attr, values in attribute_values.items():\n",
      "            templates[attr] = {}\n",
      "            for idx, value in enumerate(values):\n",
      "                templates[attr][idx] = [\n",
      "                    f\"a photo of a person with {value} appearance\",\n",
      "                    f\"a {value} person\",\n",
      "                    f\"a face with {value} features\",\n",
      "                    f\"this person appears to be {value}\",\n",
      "                    f\"a portrait showing {value} characteristics\"\n",
      "                ]\n",
      "\n",
      "        self.templates = templates\n",
      "        return templates\n",
      "\n",
      "    def classify_images(self,\n",
      "                       img_dir: str,\n",
      "                       attribute_values: Dict[str, List[str]],\n",
      "                       batch_size: int = 8) -> pd.DataFrame:\n",
      "        \"\"\"Classify images according to specified attributes using CLIP\"\"\"\n",
      "        if not os.path.exists(img_dir):\n",
      "            raise ValueError(f\"Directory not found: {img_dir}\")\n",
      "\n",
      "        # Create or get templates\n",
      "        if not self.templates:\n",
      "            print(\"Generating templates...\")\n",
      "            self.create_text_templates(attribute_values)\n",
      "            # Print templates once at initialization\n",
      "            self.print_templates()\n",
      "\n",
      "        print(f\"\\nScanning directory: {img_dir}\")\n",
      "        image_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
      "\n",
      "        if not image_files:\n",
      "            raise ValueError(f\"No image files found in {img_dir}\")\n",
      "\n",
      "        print(f\"Found {len(image_files)} images\")\n",
      "\n",
      "        results = {'image_id': []}\n",
      "        for attr in attribute_values.keys():\n",
      "            results[attr] = []\n",
      "\n",
      "        # Process images in batches\n",
      "        for i in tqdm(range(0, len(image_files), batch_size),\n",
      "                     desc=\"Classifying images\",\n",
      "                     position=0,\n",
      "                     leave=True):\n",
      "            batch_files = image_files[i:i + batch_size]\n",
      "            batch_images = []\n",
      "            valid_files = []\n",
      "\n",
      "            # Load and preprocess images\n",
      "            for img_file in batch_files:\n",
      "                try:\n",
      "                    img_path = os.path.join(img_dir, img_file)\n",
      "                    image = Image.open(img_path).convert('RGB')\n",
      "                    processed_image = self.preprocess(image)\n",
      "                    batch_images.append(processed_image)\n",
      "                    valid_files.append(img_file)\n",
      "                except Exception as e:\n",
      "                    if self.debug:\n",
      "                        print(f\"Error processing {img_file}: {str(e)}\")\n",
      "                    continue\n",
      "\n",
      "            if not batch_images:\n",
      "                continue\n",
      "\n",
      "            try:\n",
      "                image_batch = torch.stack(batch_images).to(self.device)\n",
      "\n",
      "                with torch.no_grad():\n",
      "                    image_features = self.model.encode_image(image_batch)\n",
      "                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
      "\n",
      "                    for attr, values in attribute_values.items():\n",
      "                        try:\n",
      "                            if self.debug:\n",
      "                                print(f\"\\nProcessing attribute: {attr}\")\n",
      "                                print(f\"Values: {values}\")\n",
      "\n",
      "                            all_text_features = []\n",
      "                            templates = self.templates[attr]\n",
      "\n",
      "                            for idx, value in enumerate(values):\n",
      "                                try:\n",
      "                                    # Try both integer and string keys\n",
      "                                    if idx in templates:\n",
      "                                        key = idx\n",
      "                                    elif str(idx) in templates:\n",
      "                                        key = str(idx)\n",
      "                                    else:\n",
      "                                        raise KeyError(f\"Neither key {idx} nor '{idx}' found in templates\")\n",
      "\n",
      "                                    texts = templates[key]\n",
      "                                    text_tokens = clip.tokenize(texts).to(self.device)\n",
      "                                    text_features = self.model.encode_text(text_tokens)\n",
      "                                    text_features = text_features.mean(dim=0, keepdim=True)\n",
      "                                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
      "                                    all_text_features.append(text_features)\n",
      "\n",
      "                                except Exception as e:\n",
      "                                    if self.debug:\n",
      "                                        print(f\"Error processing value {value}:\")\n",
      "                                        print(f\"Full error: {str(e)}\")\n",
      "                                        import traceback\n",
      "                                        traceback.print_exc()\n",
      "                                    raise\n",
      "\n",
      "                            text_features = torch.cat(all_text_features)\n",
      "                            similarity = (100.0 * image_features @ text_features.T)\n",
      "                            predictions = similarity.softmax(dim=-1).cpu().numpy()\n",
      "                            class_indices = predictions.argmax(axis=1)\n",
      "\n",
      "                            results[attr].extend(class_indices.tolist())\n",
      "\n",
      "                        except Exception as e:\n",
      "                            if self.debug:\n",
      "                                print(f\"Error processing attribute {attr}:\")\n",
      "                                print(f\"Full error: {str(e)}\")\n",
      "                                import traceback\n",
      "                                traceback.print_exc()\n",
      "                            raise\n",
      "\n",
      "                results['image_id'].extend(valid_files)\n",
      "\n",
      "            except Exception as e:\n",
      "                if self.debug:\n",
      "                    print(f\"Error processing batch:\")\n",
      "                    print(f\"Full error: {str(e)}\")\n",
      "                    import traceback\n",
      "                    traceback.print_exc()\n",
      "                continue\n",
      "\n",
      "        df = pd.DataFrame(results)\n",
      "        self._print_distribution_statistics(df)\n",
      "        return df\n",
      "\n",
      "    def _print_distribution_statistics(self, df: pd.DataFrame):\n",
      "        \"\"\"Print distribution statistics for each attribute\"\"\"\n",
      "        print(f\"\\nProcessed {len(df)} images successfully\")\n",
      "        print(\"\\nAttribute distribution:\")\n",
      "\n",
      "        for attr, mapping in self.attribute_mappings.items():\n",
      "            print(f\"\\n{attr.capitalize()} Distribution:\")\n",
      "            print(\"-\" * 30)\n",
      "\n",
      "            counts = df[attr].value_counts().sort_index()\n",
      "            total = len(df)\n",
      "\n",
      "            for idx in sorted(mapping.keys()):\n",
      "                count = counts.get(idx, 0)\n",
      "                percentage = (count / total) * 100\n",
      "                print(f\"{mapping[idx]:<25}: {count:>6,} ({percentage:>6.1f}%)\")\n",
      "\n",
      "\n",
      "def prepare_attributes_for_vae(df: pd.DataFrame,\n",
      "                               attribute_mapping: Dict[str, List[str]]) -> pd.DataFrame:\n",
      "    \"\"\"Prepare the classified attributes for use in the VAE\"\"\"\n",
      "    vae_df = df.copy()\n",
      "\n",
      "    for attr, values in attribute_mapping.items():\n",
      "        vae_df[attr] = vae_df[attr].astype(int)\n",
      "\n",
      "    return vae_df\n",
      "----------------------------------------\n",
      "File: plots.py\n",
      "\n",
      "import pandas as pd\n",
      "import torch\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "from typing import Dict, List, Tuple, Optional\n",
      "import seaborn as sns\n",
      "from utils import attribute_to_text, get_attribute_changes\n",
      "\n",
      "\n",
      "def plot_reconstructions(original_images: torch.Tensor,\n",
      "                         reconstructed_images: torch.Tensor,\n",
      "                         attrs: torch.Tensor,\n",
      "                         attribute_names: List[str],\n",
      "                         attribute_values: Dict[str, List[str]],\n",
      "                         epoch: int,\n",
      "                         save_dir: str,\n",
      "                         max_samples: int = 5) -> None:\n",
      "    \"\"\"\n",
      "    Plot original and reconstructed images with their attributes.\n",
      "\n",
      "    Args:\n",
      "        original_images: Tensor of original images\n",
      "        reconstructed_images: Tensor of reconstructed images\n",
      "        attrs: Tensor of attribute values\n",
      "        attribute_names: List of attribute names\n",
      "        attribute_values: Dictionary mapping attributes to possible values\n",
      "        epoch: Current epoch number\n",
      "        save_dir: Directory to save plots\n",
      "        max_samples: Maximum number of samples to plot\n",
      "    \"\"\"\n",
      "    num_samples = min(max_samples, original_images.size(0))\n",
      "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 3 * num_samples))\n",
      "    fig.suptitle(f'Original vs Reconstructed Images - Epoch {epoch}')\n",
      "\n",
      "    # Get attribute descriptions\n",
      "    descriptions = attribute_to_text(attrs[:num_samples], attribute_names, attribute_values)\n",
      "\n",
      "    for idx in range(num_samples):\n",
      "        # Original image\n",
      "        orig_img = original_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "        axes[idx, 0].imshow(orig_img)\n",
      "        axes[idx, 0].set_title('Original')\n",
      "        axes[idx, 0].axis('off')\n",
      "\n",
      "        # Reconstructed image\n",
      "        recon_img = reconstructed_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "        axes[idx, 1].imshow(recon_img)\n",
      "        axes[idx, 1].set_title('Reconstructed')\n",
      "        axes[idx, 1].axis('off')\n",
      "\n",
      "        # Add attributes as text\n",
      "        plt.figtext(0.5, 0.98 - (idx * 0.2), descriptions[idx],\n",
      "                    ha='center', va='top', bbox=dict(facecolor='white', alpha=0.8))\n",
      "\n",
      "    plt.tight_layout()\n",
      "    os.makedirs(save_dir, exist_ok=True)\n",
      "    plt.savefig(os.path.join(save_dir, f'reconstructions_epoch_{epoch}.png'))\n",
      "    plt.close()\n",
      "\n",
      "\n",
      "def plot_training_progress(loss_history: Dict[str, List[float]], save_dir: str) -> None:\n",
      "    \"\"\"\n",
      "    Plot training progress including all loss components.\n",
      "\n",
      "    Args:\n",
      "        loss_history: Dictionary containing loss values over time\n",
      "        save_dir: Directory to save plots\n",
      "    \"\"\"\n",
      "    plt.figure(figsize=(12, 8))\n",
      "\n",
      "    # Plot all loss components\n",
      "    for loss_name, values in loss_history.items():\n",
      "        if loss_name != 'beta':  # Plot beta separately\n",
      "            plt.plot(values, label=loss_name.capitalize())\n",
      "\n",
      "    plt.title('Training Losses Over Time')\n",
      "    plt.xlabel('Epoch')\n",
      "    plt.ylabel('Loss')\n",
      "    plt.yscale('log')\n",
      "    plt.grid(True)\n",
      "    plt.legend()\n",
      "\n",
      "    # Save the plot\n",
      "    os.makedirs(save_dir, exist_ok=True)\n",
      "    plt.savefig(os.path.join(save_dir, 'training_losses.png'))\n",
      "    plt.close()\n",
      "\n",
      "    # Plot beta schedule separately if it exists\n",
      "    if 'beta' in loss_history:\n",
      "        plt.figure(figsize=(12, 4))\n",
      "        plt.plot(loss_history['beta'], color='purple')\n",
      "        plt.title('Beta Schedule Over Time')\n",
      "        plt.xlabel('Epoch')\n",
      "        plt.ylabel('Beta')\n",
      "        plt.grid(True)\n",
      "        plt.savefig(os.path.join(save_dir, 'beta_schedule.png'))\n",
      "        plt.close()\n",
      "\n",
      "\n",
      "def plot_reconstructions_with_perturbations(original_images: torch.Tensor,\n",
      "                                            reconstructed_images: torch.Tensor,\n",
      "                                            perturbed_images: torch.Tensor,\n",
      "                                            orig_attrs: torch.Tensor,\n",
      "                                            perturbed_attrs: torch.Tensor,\n",
      "                                            attribute_names: List[str],\n",
      "                                            attribute_values: Dict[str, List[str]],\n",
      "                                            epoch: int,\n",
      "                                            save_dir: str,\n",
      "                                            max_samples: int = 5) -> None:\n",
      "    \"\"\"\n",
      "    Plot original, reconstructed, and attribute-perturbed images.\n",
      "\n",
      "    Args:\n",
      "        original_images: Original input images\n",
      "        reconstructed_images: VAE reconstructed images\n",
      "        perturbed_images: Images with perturbed attributes\n",
      "        orig_attrs: Original attributes\n",
      "        perturbed_attrs: Perturbed attributes\n",
      "        attribute_names: List of attribute names\n",
      "        attribute_values: Dictionary mapping attributes to possible values\n",
      "        epoch: Current training epoch\n",
      "        save_dir: Directory to save visualizations\n",
      "        max_samples: Maximum number of samples to plot\n",
      "    \"\"\"\n",
      "    num_samples = min(max_samples, original_images.size(0))\n",
      "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 4 * num_samples))\n",
      "    fig.suptitle(f'Original vs Reconstructed vs Perturbed Images - Epoch {epoch}')\n",
      "\n",
      "    # Get attribute changes\n",
      "    changes = get_attribute_changes(orig_attrs[:num_samples],\n",
      "                                    perturbed_attrs[:num_samples],\n",
      "                                    attribute_names,\n",
      "                                    attribute_values)\n",
      "\n",
      "    for idx in range(num_samples):\n",
      "        # Original image\n",
      "        orig_img = original_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "        axes[idx, 0].imshow(orig_img)\n",
      "        axes[idx, 0].set_title('Original')\n",
      "        axes[idx, 0].axis('off')\n",
      "\n",
      "        # Reconstructed image\n",
      "        recon_img = reconstructed_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "        axes[idx, 1].imshow(recon_img)\n",
      "        axes[idx, 1].set_title('Reconstructed')\n",
      "        axes[idx, 1].axis('off')\n",
      "\n",
      "        # Perturbed image\n",
      "        pert_img = perturbed_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "        axes[idx, 2].imshow(pert_img)\n",
      "\n",
      "        # Create change description\n",
      "        change_text = []\n",
      "        for attr_name, change in changes[idx].items():\n",
      "            change_text.append(f\"{attr_name}: {change}\")\n",
      "        change_desc = \" | \".join(change_text)\n",
      "\n",
      "        axes[idx, 2].set_title('Perturbed\\n' + change_desc, fontsize=10)\n",
      "        axes[idx, 2].axis('off')\n",
      "\n",
      "    plt.tight_layout()\n",
      "    os.makedirs(save_dir, exist_ok=True)\n",
      "    plt.savefig(os.path.join(save_dir, f'perturbations_epoch_{epoch}.png'))\n",
      "    plt.close()\n",
      "\n",
      "\n",
      "def plot_attribute_distributions(df: pd.DataFrame,\n",
      "                                 attribute_names: List[str],\n",
      "                                 attribute_values: Dict[str, List[str]],\n",
      "                                 save_dir: str) -> None:\n",
      "    \"\"\"\n",
      "    Plot distribution of attributes in the dataset.\n",
      "\n",
      "    Args:\n",
      "        df: DataFrame containing attribute values\n",
      "        attribute_names: List of attribute names\n",
      "        attribute_values: Dictionary mapping attributes to possible values\n",
      "        save_dir: Directory to save plots\n",
      "    \"\"\"\n",
      "    num_attrs = len(attribute_names)\n",
      "    fig = plt.figure(figsize=(15, 5 * ((num_attrs + 1) // 2)))\n",
      "\n",
      "    for idx, attr_name in enumerate(attribute_names, 1):\n",
      "        plt.subplot(((num_attrs + 1) // 2), 2, idx)\n",
      "\n",
      "        # Get value counts\n",
      "        value_counts = df[attr_name].value_counts().sort_index()\n",
      "        values = [attribute_values[attr_name][i] for i in value_counts.index]\n",
      "\n",
      "        # Create bar plot\n",
      "        sns.barplot(x=values, y=value_counts.values)\n",
      "        plt.title(f'Distribution of {attr_name}')\n",
      "        plt.xticks(rotation=45, ha='right')\n",
      "        plt.ylabel('Count')\n",
      "\n",
      "    plt.tight_layout()\n",
      "    os.makedirs(save_dir, exist_ok=True)\n",
      "    plt.savefig(os.path.join(save_dir, 'attribute_distributions.png'))\n",
      "    plt.close()\n",
      "\n",
      "\n",
      "def plot_latent_space(latent_vectors: torch.Tensor,\n",
      "                      attrs: torch.Tensor,\n",
      "                      attribute_names: List[str],\n",
      "                      attribute_values: Dict[str, List[str]],\n",
      "                      save_dir: str) -> None:\n",
      "    \"\"\"\n",
      "    Plot 2D visualization of latent space colored by attributes.\n",
      "\n",
      "    Args:\n",
      "        latent_vectors: Encoded latent vectors\n",
      "        attrs: Attribute tensors\n",
      "        attribute_names: List of attribute names\n",
      "        attribute_values: Dictionary mapping attributes to possible values\n",
      "        save_dir: Directory to save plots\n",
      "    \"\"\"\n",
      "    # Reduce dimensionality to 2D using PCA\n",
      "    from sklearn.decomposition import PCA\n",
      "    pca = PCA(n_components=2)\n",
      "    latent_2d = pca.fit_transform(latent_vectors.cpu().numpy())\n",
      "\n",
      "    # Create a plot for each attribute\n",
      "    for attr_idx, attr_name in enumerate(attribute_names):\n",
      "        plt.figure(figsize=(10, 8))\n",
      "\n",
      "        # Get attribute values for coloring\n",
      "        attr_vals = attrs[:, attr_idx].cpu().numpy()\n",
      "        values = attribute_values[attr_name]\n",
      "\n",
      "        # Create scatter plot\n",
      "        scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1],\n",
      "                              c=attr_vals, cmap='tab10',\n",
      "                              alpha=0.6)\n",
      "\n",
      "        # Add legend\n",
      "        legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
      "                                      markerfacecolor=scatter.cmap(scatter.norm(i)),\n",
      "                                      label=val, markersize=10)\n",
      "                           for i, val in enumerate(values)]\n",
      "        plt.legend(handles=legend_elements, title=attr_name)\n",
      "\n",
      "        plt.title(f'Latent Space Colored by {attr_name}')\n",
      "        plt.xlabel('First Principal Component')\n",
      "        plt.ylabel('Second Principal Component')\n",
      "\n",
      "        os.makedirs(save_dir, exist_ok=True)\n",
      "        plt.savefig(os.path.join(save_dir, f'latent_space_{attr_name}.png'))\n",
      "        plt.close()\n",
      "\n",
      "\n",
      "def plot_attribute_correlations(df: pd.DataFrame,\n",
      "                                attribute_names: List[str],\n",
      "                                save_dir: str) -> None:\n",
      "    \"\"\"\n",
      "    Plot correlation matrix between attributes.\n",
      "\n",
      "    Args:\n",
      "        df: DataFrame containing attribute values\n",
      "        attribute_names: List of attribute names\n",
      "        save_dir: Directory to save plots\n",
      "    \"\"\"\n",
      "    # Calculate correlations\n",
      "    corr_matrix = df[attribute_names].corr()\n",
      "\n",
      "    # Create heatmap\n",
      "    plt.figure(figsize=(10, 8))\n",
      "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
      "                square=True, fmt='.2f', cbar_kws={'label': 'Correlation'})\n",
      "\n",
      "    plt.title('Attribute Correlations')\n",
      "    plt.tight_layout()\n",
      "\n",
      "    os.makedirs(save_dir, exist_ok=True)\n",
      "    plt.savefig(os.path.join(save_dir, 'attribute_correlations.png'))\n",
      "    plt.close()\n",
      "\n",
      "\n",
      "def plot_clip_confidence_distribution(confidence_scores: torch.Tensor,\n",
      "                                      attribute_names: List[str],\n",
      "                                      save_dir: str) -> None:\n",
      "    \"\"\"\n",
      "    Plot distribution of CLIP confidence scores for each attribute.\n",
      "\n",
      "    Args:\n",
      "        confidence_scores: Tensor of CLIP confidence scores\n",
      "        attribute_names: List of attribute names\n",
      "        save_dir: Directory to save plots\n",
      "    \"\"\"\n",
      "    num_attrs = len(attribute_names)\n",
      "    fig = plt.figure(figsize=(15, 5 * ((num_attrs + 1) // 2)))\n",
      "\n",
      "    for idx, attr_name in enumerate(attribute_names, 1):\n",
      "        plt.subplot(((num_attrs + 1) // 2), 2, idx)\n",
      "\n",
      "        # Get confidence scores for this attribute\n",
      "        scores = confidence_scores[:, idx].cpu().numpy()\n",
      "\n",
      "        # Create histogram\n",
      "        sns.histplot(scores, bins=50)\n",
      "        plt.title(f'CLIP Confidence Distribution for {attr_name}')\n",
      "        plt.xlabel('Confidence Score')\n",
      "        plt.ylabel('Count')\n",
      "\n",
      "    plt.tight_layout()\n",
      "    os.makedirs(save_dir, exist_ok=True)\n",
      "    plt.savefig(os.path.join(save_dir, 'clip_confidence_distributions.png'))\n",
      "    plt.close()\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad5b344db8aa9341"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
