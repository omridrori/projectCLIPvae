{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T18:07:43.322852Z",
     "start_time": "2025-02-09T18:07:43.244216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def generate_py_file_string(directory=\".\"):\n",
    "    result = \"\"\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    result += f\"File: {file}\\n\\n{content}\\n{'-' * 40}\\n\"\n",
    "                except Exception as e:\n",
    "                    result += f\"File: {file}\\n\\nError reading file: {e}\\n{'-' * 40}\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output = generate_py_file_string()\n",
    "    print(output)\n",
    "    "
   ],
   "id": "e3f36a820c575835",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: models.py\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "from torch import nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class Encoder(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Encoder, self).__init__()\n",
      "        # channels_in ,  channels_out, kernel_size, stride , padding,\n",
      "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1)\n",
      "        self.conv2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
      "        self.conv3 = nn.Conv2d(64, 64, 4, 2, 1)\n",
      "        self.conv4 = nn.Conv2d(64, 128, 4, 2, 1)\n",
      "        self.maxp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "\n",
      "    def forward(self, x):\n",
      "        out = self.conv1(x)\n",
      "        out = self.maxp1(out)\n",
      "        out = F.relu(out)\n",
      "        out = self.conv2(out)\n",
      "        out = self.maxp2(out)\n",
      "        out = F.relu(out)\n",
      "        out = self.conv3(out)\n",
      "        out = self.maxp3(out)\n",
      "        out = F.relu(out)\n",
      "        out = self.conv4(out)\n",
      "        out = self.maxp4(out)\n",
      "        out = F.relu(out)\n",
      "        return out.view(out.shape[0], -1)\n",
      "\n",
      "\n",
      "class Decoder(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Decoder, self).__init__()\n",
      "        # channels_in ,  channels_out, kernel_size, stride , padding,\n",
      "        self.transconv1 = nn.ConvTranspose2d(64 + 40, 64, 8, 4, 2)\n",
      "        self.transconv2 = nn.ConvTranspose2d(64, 64, 8, 4, 2)\n",
      "        self.transconv3 = nn.ConvTranspose2d(64, 64, 4, 2, 1)\n",
      "        self.transconv4 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
      "\n",
      "\n",
      "        self.hairEmbedding = nn.Embedding(4, 10)\n",
      "        self.beardEmbedding = nn.Embedding(2, 10)\n",
      "        self.genderEmbedding = nn.Embedding(2, 10)\n",
      "        self.paleSkinEmbedding = nn.Embedding(2, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        z = x[:, :64]\n",
      "        hair = self.hairEmbedding(x[:, 64].long())\n",
      "        paleSkin = self.paleSkinEmbedding(x[:, 65].long())\n",
      "        gender = self.genderEmbedding(x[:, 66].long())\n",
      "        beard = self.beardEmbedding(x[:, 67].long())\n",
      "        \"\"\"\n",
      "        Concating the embeddings and the encoded image\n",
      "        \"\"\"\n",
      "        z = torch.cat([z, hair, beard, gender, paleSkin], dim=1)\n",
      "\n",
      "        out = self.transconv1(z.view(z.shape[0], z.shape[1], 1, 1))\n",
      "        out = F.relu(out)\n",
      "        out = self.transconv2(out)\n",
      "        out = F.relu(out)\n",
      "\n",
      "        out = self.transconv3(out)\n",
      "        out = F.relu(out)\n",
      "\n",
      "        out = self.transconv4(out)\n",
      "        out = F.sigmoid(out)\n",
      "\n",
      "        return out\n",
      "\n",
      "\n",
      "class CVAE(nn.Module):\n",
      "    def __init__(self, encoder, decoder):\n",
      "        super(CVAE, self).__init__()\n",
      "        self.encoder = encoder()\n",
      "        self.decoder = decoder()\n",
      "\n",
      "    def forward(self, x, attrs):\n",
      "        h = self.encoder(x)\n",
      "\n",
      "        mu = h[:, :64]\n",
      "        logvar = h[:, 64:]\n",
      "        # this part is for the reparameterization trick\n",
      "        s = torch.exp(logvar)\n",
      "        eps = torch.randn_like(s)\n",
      "        z = s * eps + mu\n",
      "\n",
      "        z = torch.cat([z, attrs], dim=1)\n",
      "        out = self.decoder(z)\n",
      "        return out, mu, logvar\n",
      "\n",
      "\n",
      "def loss_function(recon_x, x, mu, logvar, beta):\n",
      "    \"\"\"\n",
      "    Compute VAE loss with BCE reconstruction loss + KL divergence with beta scheduling\n",
      "\n",
      "    Args:\n",
      "        recon_x: reconstructed input\n",
      "        x: original input\n",
      "        mu: mean of the latent distribution\n",
      "        logvar: log variance of the latent distribution\n",
      "        beta: weight for the KL divergence term (from scheduler)\n",
      "    \"\"\"\n",
      "    # BCE reconstruction loss\n",
      "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
      "    recon_loss = recon_loss / (x.size(0) * 3 * 64 * 64)  # Normalize by batch size and dimensions\n",
      "\n",
      "    # KL divergence\n",
      "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
      "    KLD = KLD / (x.size(0) * 3 * 64 * 64)  # Normalize by batch size and dimensions\n",
      "\n",
      "    # Apply beta weighting to KL divergence\n",
      "    total_loss = recon_loss + beta * KLD\n",
      "    return total_loss, recon_loss, KLD\n",
      "\n",
      "\n",
      "class CosineScheduler:\n",
      "    def __init__(self, start_value, end_value, num_cycles, num_epochs):\n",
      "        \"\"\"\n",
      "        Cosine annealing scheduler for beta value\n",
      "\n",
      "        Args:\n",
      "            start_value: Initial beta value\n",
      "            end_value: Final beta value\n",
      "            num_cycles: Number of cycles for the cosine schedule\n",
      "            num_epochs: Total number of epochs\n",
      "        \"\"\"\n",
      "        self.start_value = start_value\n",
      "        self.end_value = end_value\n",
      "        self.num_cycles = num_cycles\n",
      "        self.num_epochs = num_epochs\n",
      "\n",
      "    def get_value(self, epoch):\n",
      "        \"\"\"\n",
      "        Calculate the current beta value based on the epoch\n",
      "        \"\"\"\n",
      "        # Calculate the progress within the current cycle\n",
      "        cycle_length = self.num_epochs / self.num_cycles\n",
      "        cycle_progress = (epoch % cycle_length) / cycle_length\n",
      "\n",
      "        # Calculate cosine value (0 to 1)\n",
      "        cosine_value = 0.5 * (1 + np.cos(np.pi * cycle_progress))\n",
      "\n",
      "        # Interpolate between start and end values\n",
      "        current_value = self.end_value + (self.start_value - self.end_value) * cosine_value\n",
      "        return current_value\n",
      "----------------------------------------\n",
      "File: utils.py\n",
      "\n",
      "def ceil(a,b):\n",
      "    return -(-a//b)\n",
      "\n",
      "----------------------------------------\n",
      "File: celeba_project.py\n",
      "\n",
      "from PIL import Image\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import matplotlib.pyplot as plt\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "import torchvision.transforms as transforms\n",
      "import os\n",
      "from PIL import Image\n",
      "\n",
      "import torch.optim as  optim\n",
      "from tqdm import tqdm\n",
      "\n",
      "from dataset import CelebaDataset, transform\n",
      "from models import Encoder, Decoder, CVAE, loss_function, CosineScheduler\n",
      "from plots import plot_reconstructions\n",
      "\n",
      "if torch.cuda.is_available():\n",
      "  dev = \"cuda:0\"\n",
      "  print(\"gpu up\")\n",
      "else:\n",
      "  dev = \"cpu\"\n",
      "device = torch.device(dev)\n",
      "\n",
      "df = pd.read_csv(\"celeba_dataset/list_attr_celeba.csv\")\n",
      "\n",
      "def haircolor(x):\n",
      "    if x[\"Blond_Hair\"] == 1:\n",
      "        return 0\n",
      "    elif x[\"Brown_Hair\"] == 1:\n",
      "        return 1\n",
      "    elif x[\"Black_Hair\"] == 1:\n",
      "        return 2\n",
      "    else:\n",
      "        return 3\n",
      "\n",
      "\n",
      "df[\"Hair_Color\"] = df.apply(haircolor, axis=1)\n",
      "\n",
      "df = df[[\"image_id\",\"Hair_Color\",'Pale_Skin',\"Male\",\"No_Beard\"]]\n",
      "df.Pale_Skin = df.Pale_Skin.apply(lambda x: max(x,0))\n",
      "df.Male = df.Male.apply(lambda x: max(x,0))\n",
      "df.No_Beard = df.No_Beard.apply(lambda x: max(x,0))\n",
      "\n",
      "\n",
      "\n",
      "# Initialize dataset and dataloader\n",
      "dataset = CelebaDataset(\n",
      "    df=df,\n",
      "    img_dir = \"/home/omrid/Desktop/jungo /projectCLIPvae/celeba_dataset/img_align_celeba/img_align_celeba/\",\n",
      "    transform=transform\n",
      ")\n",
      "\n",
      "dataloader = DataLoader(\n",
      "    dataset,\n",
      "    batch_size=128,\n",
      "    shuffle=True,\n",
      "    num_workers=4,  # Parallel data loading\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "vae = CVAE(Encoder, Decoder)\n",
      "vae.to(device)\n",
      "\n",
      "\n",
      "def train_vae(vae, dataloader, optimizer, device, num_epochs=1201, save_interval=100):\n",
      "    \"\"\"\n",
      "    Train the VAE model using BCE loss with beta scheduling\n",
      "    \"\"\"\n",
      "    vae.train()\n",
      "    loss_history = {\n",
      "        'total': [],\n",
      "        'reconstruction': [],\n",
      "        'kl': [],\n",
      "        'beta': []\n",
      "    }\n",
      "\n",
      "    # Initialize beta scheduler\n",
      "    beta_scheduler = CosineScheduler(\n",
      "        start_value=0.0,  # Start with low KL weight\n",
      "        end_value=1.0,  # End with full KL weight\n",
      "        num_cycles=4,  # Number of cycles during training\n",
      "        num_epochs=num_epochs\n",
      "    )\n",
      "\n",
      "    for epoch in range(num_epochs):\n",
      "        pbar = tqdm(total=len(dataloader), desc=f'Epoch {epoch}')\n",
      "        epoch_total_loss = 0\n",
      "        epoch_recon_loss = 0\n",
      "        epoch_kl_loss = 0\n",
      "\n",
      "        # Get current beta value\n",
      "        current_beta = beta_scheduler.get_value(epoch)\n",
      "\n",
      "        # Store first batch for visualization\n",
      "        vis_batch = None\n",
      "\n",
      "        for batch_idx, (images, attrs) in enumerate(dataloader):\n",
      "            if batch_idx == 0:\n",
      "                vis_batch = (images[:5].clone(), attrs[:5].clone())\n",
      "\n",
      "            images = images.to(device)\n",
      "            attrs = attrs.to(device)\n",
      "\n",
      "            optimizer.zero_grad()\n",
      "            recon_images, mu, logvar = vae(images, attrs)\n",
      "\n",
      "            # Compute losses with current beta value\n",
      "            total_loss, recon_loss, kl_loss = loss_function(recon_images, images, mu, logvar, current_beta)\n",
      "\n",
      "            total_loss.backward()\n",
      "            optimizer.step()\n",
      "\n",
      "            # Accumulate losses\n",
      "            epoch_total_loss += total_loss.item()\n",
      "            epoch_recon_loss += recon_loss.item()\n",
      "            epoch_kl_loss += kl_loss.item()\n",
      "\n",
      "            pbar.update(1)\n",
      "\n",
      "        pbar.close()\n",
      "\n",
      "        # Calculate averages\n",
      "        avg_total = epoch_total_loss / len(dataloader)\n",
      "        avg_recon = epoch_recon_loss / len(dataloader)\n",
      "        avg_kl = epoch_kl_loss / len(dataloader)\n",
      "\n",
      "        # Store in history\n",
      "        loss_history['total'].append(avg_total)\n",
      "        loss_history['reconstruction'].append(avg_recon)\n",
      "        loss_history['kl'].append(avg_kl)\n",
      "        loss_history['beta'].append(current_beta)\n",
      "\n",
      "        print(f\"\\nEpoch {epoch} Summary:\")\n",
      "        print(f\"Total Loss: {avg_total:.6f}\")\n",
      "        print(f\"BCE Loss: {avg_recon:.6f}\")\n",
      "        print(f\"KL Loss: {avg_kl:.6f}\")\n",
      "        print(f\"Current Beta: {current_beta:.6f}\\n\")\n",
      "\n",
      "        # Generate and save reconstructions\n",
      "        if vis_batch is not None:\n",
      "            vae.eval()\n",
      "            with torch.no_grad():\n",
      "                vis_images, vis_attrs = vis_batch\n",
      "                vis_images = vis_images.to(device)\n",
      "                vis_attrs = vis_attrs.to(device)\n",
      "                recon_images, _, _ = vae(vis_images, vis_attrs)\n",
      "                plot_reconstructions(vis_images, recon_images, vis_attrs, epoch)\n",
      "            vae.train()\n",
      "\n",
      "        # Save checkpoint at intervals\n",
      "        if epoch % save_interval == 0:\n",
      "            torch.save({\n",
      "                'epoch': epoch,\n",
      "                'model_state_dict': vae.state_dict(),\n",
      "                'optimizer_state_dict': optimizer.state_dict(),\n",
      "                'loss': avg_total,\n",
      "                'beta': current_beta\n",
      "            }, f\"vae_checkpoint_epoch_{epoch}.pt\")\n",
      "\n",
      "    return loss_history\n",
      "\n",
      "\n",
      "# Setup training\n",
      "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "vae = vae.to(device)\n",
      "optimizer =  optim.Adam(vae.parameters(), lr=0.0001)  # Adjust learning rate if needed\n",
      "\n",
      "# Train the model\n",
      "loss_history = train_vae(\n",
      "    vae=vae,\n",
      "    dataloader=dataloader,\n",
      "    optimizer=optimizer,\n",
      "    device=device,\n",
      "    num_epochs=1201,\n",
      "    save_interval=100\n",
      ")\n",
      "# Plot training progress\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(loss_history)\n",
      "plt.title('Training Loss Over Time')\n",
      "plt.xlabel('Epoch')\n",
      "plt.ylabel('Loss')\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "File: dataset.py\n",
      "\n",
      "import os\n",
      "\n",
      "import torch\n",
      "from PIL import Image\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "import torchvision.transforms as transforms\n",
      "\n",
      "\n",
      "class CelebaDataset(Dataset):\n",
      "    def __init__(self, df, img_dir, transform=None):\n",
      "        self.df = df\n",
      "        self.img_dir = img_dir\n",
      "        self.transform = transform\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.df)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        img_name = self.df.iloc[idx]['image_id']\n",
      "        img_path = os.path.join(self.img_dir, img_name)\n",
      "        image = Image.open(img_path)\n",
      "\n",
      "        if self.transform:\n",
      "            image = self.transform(image)\n",
      "\n",
      "        # Extract attributes\n",
      "        attrs = self.df.iloc[idx][['Hair_Color', 'Pale_Skin', 'Male', 'No_Beard']].values\n",
      "        attrs = torch.tensor(attrs.astype('float32'))\n",
      "\n",
      "        return image, attrs\n",
      "\n",
      "\n",
      "# Define transforms\n",
      "transform = transforms.Compose([\n",
      "    transforms.Resize((64, 64)),\n",
      "    transforms.ToTensor(),  # Automatically scales to [0,1] and converts to (C, H, W)\n",
      "])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "File: plots.py\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "\n",
      "def plot_reconstructions(original_images, reconstructed_images, attrs, epoch):\n",
      "    \"\"\"\n",
      "    Plot 5 original and reconstructed images side by side with their attributes\n",
      "    \"\"\"\n",
      "    fig, axes = plt.subplots(5, 2, figsize=(12, 15))\n",
      "    fig.suptitle(f'Original vs Reconstructed Images - Epoch {epoch}')\n",
      "\n",
      "    # Labels for attributes\n",
      "    hair_labels = ['Blond', 'Brown', 'Black', 'Other']\n",
      "\n",
      "    for idx in range(5):\n",
      "        if idx < len(original_images):\n",
      "            # Original image\n",
      "            orig_img = original_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 0].imshow(orig_img)\n",
      "            axes[idx, 0].set_title('Original')\n",
      "\n",
      "            # Reconstructed image\n",
      "            recon_img = reconstructed_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 1].imshow(recon_img)\n",
      "            axes[idx, 1].set_title('Reconstructed')\n",
      "\n",
      "            # Get attributes for this image\n",
      "            attr = attrs[idx]\n",
      "            attr_text = f'Hair: {hair_labels[int(attr[0])]} | '\n",
      "            attr_text += f'Pale Skin: {bool(attr[1])} | '\n",
      "            attr_text += f'Male: {bool(attr[2])} | '\n",
      "            attr_text += f'No Beard: {bool(attr[3])}'\n",
      "\n",
      "            # Add attributes as text below the images\n",
      "            plt.figtext(0.5, 0.98 - (idx * 0.2), attr_text,\n",
      "                        ha='center', va='top', bbox=dict(facecolor='white', alpha=0.8))\n",
      "\n",
      "            # Remove axes\n",
      "            axes[idx, 0].axis('off')\n",
      "            axes[idx, 1].axis('off')\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(f'/home/omrid/Desktop/jungo /projectCLIPvae/plot_reconstructions/reconstructions_epoch_{epoch}.png')\n",
      "    plt.close()\n",
      "\n",
      "\n",
      "def plot_training_progress(loss_history):\n",
      "    \"\"\"\n",
      "    Plot training progress including losses and beta values\n",
      "    \"\"\"\n",
      "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
      "\n",
      "    # Plot losses\n",
      "    ax1.plot(loss_history['total'], label='Total Loss')\n",
      "    ax1.plot(loss_history['reconstruction'], label='Reconstruction Loss')\n",
      "    ax1.plot(loss_history['kl'], label='KL Loss')\n",
      "    ax1.set_title('Training Losses Over Time')\n",
      "    ax1.set_xlabel('Epoch')\n",
      "    ax1.set_ylabel('Loss')\n",
      "    ax1.grid(True)\n",
      "    ax1.legend()\n",
      "\n",
      "    # Plot beta values\n",
      "    ax2.plot(loss_history['beta'], label='Beta Value', color='purple')\n",
      "    ax2.set_title('Beta Schedule Over Time')\n",
      "    ax2.set_xlabel('Epoch')\n",
      "    ax2.set_ylabel('Beta')\n",
      "    ax2.grid(True)\n",
      "    ax2.legend()\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.show()\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad5b344db8aa9341"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
