{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T15:35:21.317953Z",
     "start_time": "2025-02-13T15:35:21.105433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def generate_py_file_string(directory=\".\"):\n",
    "    result = \"\"\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    result += f\"File: {file}\\n\\n{content}\\n{'-' * 40}\\n\"\n",
    "                except Exception as e:\n",
    "                    result += f\"File: {file}\\n\\nError reading file: {e}\\n{'-' * 40}\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output = generate_py_file_string()\n",
    "    print(output)\n",
    "    "
   ],
   "id": "e3f36a820c575835",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: models.py\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "from torch import nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "from clip_loss import  CLIPAttributeConsistency\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "import torch.nn.functional as F\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "import torch.nn.functional as F\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "import torch.nn.functional as F\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class Encoder(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Encoder, self).__init__()\n",
      "        # Keep original encoder architecture\n",
      "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1)\n",
      "        self.conv2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
      "        self.conv3 = nn.Conv2d(64, 64, 4, 2, 1)\n",
      "        self.conv4 = nn.Conv2d(64, 128, 4, 2, 1)\n",
      "        self.maxp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "\n",
      "        # Add FC layers for mu and logvar\n",
      "        self.fc_mu = nn.Linear(128, 64)\n",
      "        self.fc_logvar = nn.Linear(128, 64)\n",
      "\n",
      "    def forward(self, x):\n",
      "        out = self.conv1(x)\n",
      "        out = self.maxp1(out)\n",
      "        out = F.relu(out)\n",
      "        out = self.conv2(out)\n",
      "        out = self.maxp2(out)\n",
      "        out = F.relu(out)\n",
      "        out = self.conv3(out)\n",
      "        out = self.maxp3(out)\n",
      "        out = F.relu(out)\n",
      "        out = self.conv4(out)\n",
      "        out = self.maxp4(out)\n",
      "        out = F.relu(out)\n",
      "\n",
      "        # Flatten\n",
      "        out = out.view(out.size(0), -1)\n",
      "\n",
      "        # Split into mu and logvar\n",
      "        mu = self.fc_mu(out)\n",
      "        logvar = self.fc_logvar(out)\n",
      "\n",
      "        return mu, logvar\n",
      "\n",
      "\n",
      "class Decoder(nn.Module):\n",
      "    def __init__(self, attribute_dims):\n",
      "        \"\"\"\n",
      "        attribute_dims: Dictionary of attribute names and their number of possible values\n",
      "        e.g., {\"ethnicity\": 3, \"age\": 3}\n",
      "        \"\"\"\n",
      "        super(Decoder, self).__init__()\n",
      "\n",
      "        # Core architecture remains the same\n",
      "        # Calculate total embedding dimension\n",
      "        total_embedding_dims = sum(10 for dims in attribute_dims.values())  # 10-dim embedding for each attribute\n",
      "        self.total_input_dim = 64 + total_embedding_dims\n",
      "\n",
      "        self.transconv1 = nn.ConvTranspose2d(self.total_input_dim, 64, 8, 4, 2)\n",
      "        self.transconv2 = nn.ConvTranspose2d(64, 64, 8, 4, 2)\n",
      "        self.transconv3 = nn.ConvTranspose2d(64, 64, 4, 2, 1)\n",
      "        self.transconv4 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
      "\n",
      "        # Dynamic embeddings based on attributes\n",
      "        self.embeddings = nn.ModuleDict()\n",
      "        for attr_name, num_values in attribute_dims.items():\n",
      "            self.embeddings[attr_name] = nn.Embedding(num_values, 10)\n",
      "\n",
      "        # Store attribute names in order\n",
      "        self.attribute_names = list(attribute_dims.keys())\n",
      "\n",
      "    def forward(self, x):\n",
      "\n",
      "        # Split latent vector and attributes\n",
      "        z = x[:, :64]\n",
      "        current_idx = 64\n",
      "\n",
      "        # Process each attribute through its embedding\n",
      "        embeddings = []\n",
      "        for attr_name in self.attribute_names:\n",
      "            attr_val = x[:, current_idx].long()\n",
      "            embedding = self.embeddings[attr_name](attr_val)\n",
      "            embeddings.append(embedding)\n",
      "            current_idx += 1\n",
      "\n",
      "        # Concatenate latent vector with all embeddings\n",
      "        z = torch.cat([z] + embeddings, dim=1)\n",
      "\n",
      "        # Decode\n",
      "        out = self.transconv1(z.view(z.shape[0], z.shape[1], 1, 1))\n",
      "        out = F.relu(out)\n",
      "        out = self.transconv2(out)\n",
      "        out = F.relu(out)\n",
      "        out = self.transconv3(out)\n",
      "        out = F.relu(out)\n",
      "        out = self.transconv4(out)\n",
      "        out = F.sigmoid(out)\n",
      "\n",
      "        return out\n",
      "\n",
      "\n",
      "class CVAE(nn.Module):\n",
      "    def __init__(self, encoder, decoder, attribute_dims):\n",
      "        super(CVAE, self).__init__()\n",
      "        self.encoder = encoder()\n",
      "        self.decoder = decoder(attribute_dims)\n",
      "\n",
      "    def forward(self, x, attrs):\n",
      "        # Get mu and logvar from encoder\n",
      "        mu, logvar = self.encoder(x)\n",
      "\n",
      "        # Reparameterization trick\n",
      "        std = torch.exp(0.5 * logvar)\n",
      "        eps = torch.randn_like(std)\n",
      "        z = mu + eps * std\n",
      "\n",
      "        # Concatenate with attributes\n",
      "        z = torch.cat([z, attrs], dim=1)\n",
      "\n",
      "        # Decode\n",
      "        out = self.decoder(z)\n",
      "        return out, mu, logvar\n",
      "\n",
      "def loss_function(recon_x, x, mu, logvar, vae, encoder_output, attrs,\n",
      "                      beta_vae=1.0, beta_clip=1.0, clip_consistency=None):\n",
      "    \"\"\"\n",
      "    Enhanced VAE loss function with CLIP-based attribute manipulation loss\n",
      "\n",
      "    Args:\n",
      "        recon_x: reconstructed input\n",
      "        x: original input\n",
      "        mu: mean of the latent distribution\n",
      "        logvar: log variance of the latent distribution\n",
      "        vae: VAE model\n",
      "        encoder_output: full encoder output\n",
      "        attrs: attribute tensor\n",
      "        beta_vae: weight for the KL divergence term\n",
      "        beta_clip: weight for the CLIP-based attribute loss\n",
      "        clip_consistency: CLIPAttributeConsistency instance\n",
      "    \"\"\"\n",
      "    # Original VAE losses\n",
      "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
      "    recon_loss = recon_loss / (x.size(0) * 3 * 64 * 64)\n",
      "\n",
      "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
      "    KLD = KLD / (x.size(0) * 3 * 64 * 64)\n",
      "\n",
      "    # Initialize CLIP consistency checker if not provided\n",
      "    if clip_consistency is None:\n",
      "        clip_consistency = CLIPAttributeConsistency(device=x.device)\n",
      "\n",
      "    # Compute CLIP-based attribute consistency loss\n",
      "    clip_loss = clip_consistency.compute_attribute_loss(vae, encoder_output, attrs)\n",
      "\n",
      "    # Combine all losses\n",
      "    total_loss = recon_loss + beta_vae * KLD + beta_clip * clip_loss\n",
      "\n",
      "    return total_loss, recon_loss, KLD, clip_loss\n",
      "\n",
      "\n",
      "class CosineScheduler:\n",
      "    def __init__(self, start_value, end_value, num_cycles, num_epochs):\n",
      "        \"\"\"\n",
      "        Cosine annealing scheduler for beta value\n",
      "\n",
      "        Args:\n",
      "            start_value: Initial beta value\n",
      "            end_value: Final beta value\n",
      "            num_cycles: Number of cycles for the cosine schedule\n",
      "            num_epochs: Total number of epochs\n",
      "        \"\"\"\n",
      "        self.start_value = start_value\n",
      "        self.end_value = end_value\n",
      "        self.num_cycles = num_cycles\n",
      "        self.num_epochs = num_epochs\n",
      "\n",
      "    def get_value(self, epoch):\n",
      "        \"\"\"\n",
      "        Calculate the current beta value based on the epoch\n",
      "        \"\"\"\n",
      "        # Calculate the progress within the current cycle\n",
      "        cycle_length = self.num_epochs / self.num_cycles\n",
      "        cycle_progress = (epoch % cycle_length) / cycle_length\n",
      "\n",
      "        # Calculate cosine value (0 to 1)\n",
      "        cosine_value = 0.5 * (1 + np.cos(np.pi * cycle_progress))\n",
      "\n",
      "        # Interpolate between start and end values\n",
      "        current_value = self.end_value + (self.start_value - self.end_value) * cosine_value\n",
      "        return current_value\n",
      "----------------------------------------\n",
      "File: utils.py\n",
      "\n",
      "def ceil(a,b):\n",
      "    return -(-a//b)\n",
      "\n",
      "----------------------------------------\n",
      "File: clip_loss.py\n",
      "\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "import clip\n",
      "from random import choice\n",
      "\n",
      "\n",
      "class CLIPAttributeConsistency:\n",
      "    def __init__(self, clip_model_name=\"ViT-B/32\", device=\"cuda\"):\n",
      "        self.device = device\n",
      "        self.model, self.preprocess = clip.load(clip_model_name, device=device)\n",
      "\n",
      "        self.attribute_templates = {\n",
      "            'hair_color': {\n",
      "                0: [\"a photo of a person with blonde hair\", \"a face with blonde hair\"],\n",
      "                1: [\"a photo of a person with brown hair\", \"a face with brown hair\"],\n",
      "                2: [\"a photo of a person with black hair\", \"a face with black hair\"],\n",
      "                3: [\"a photo of a person with red hair\", \"a face with red hair\"]\n",
      "            },\n",
      "            'pale_skin': {\n",
      "                0: [\"a photo of a person with dark skin\", \"a dark-skinned person\"],\n",
      "                1: [\"a photo of a person with pale skin\", \"a pale-skinned person\"]\n",
      "            },\n",
      "            'gender': {\n",
      "                0: [\"a photo of a woman\", \"a female face\"],\n",
      "                1: [\"a photo of a man\", \"a male face\"]\n",
      "            },\n",
      "            'beard': {\n",
      "                0: [\"a photo of a person without a beard\", \"a face without facial hair\"],\n",
      "                1: [\"a photo of a person with a beard\", \"a face with a beard\"]\n",
      "            }\n",
      "        }\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def get_image_features(self, images):\n",
      "        \"\"\"Get normalized CLIP image features\"\"\"\n",
      "        images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
      "        image_features = self.model.encode_image(images)\n",
      "        return image_features / image_features.norm(dim=-1, keepdim=True)\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def get_text_features(self, descriptions):\n",
      "        \"\"\"Get normalized CLIP text features for a list of descriptions\"\"\"\n",
      "        text_tokens = clip.tokenize(descriptions).to(self.device)\n",
      "        text_features = self.model.encode_text(text_tokens)\n",
      "        return text_features / text_features.norm(dim=-1, keepdim=True)\n",
      "\n",
      "    def compute_attribute_loss(self, vae, encoder_output, attrs):\n",
      "        \"\"\"\n",
      "        Compute CLIP-based attribute consistency loss for entire batch in parallel\n",
      "        \"\"\"\n",
      "        batch_size = attrs.size(0)\n",
      "\n",
      "        # Get latent representations\n",
      "        z = encoder_output[:, :64]  # [batch_size, 64]\n",
      "\n",
      "        # Randomly select attributes to manipulate for each image in batch\n",
      "        attr_indices = torch.randint(0, 4, (batch_size,), device=attrs.device)  # [batch_size]\n",
      "\n",
      "        # Create mask for multiclass (hair_color) vs binary attributes\n",
      "        is_hair_color = (attr_indices == 0)\n",
      "\n",
      "        # Get original attribute values\n",
      "        orig_attr_vals = torch.gather(attrs, 1, attr_indices.unsqueeze(1)).squeeze(1)  # [batch_size]\n",
      "\n",
      "        # Create perturbed attributes tensor\n",
      "        perturbed_attrs = attrs.clone().float()  # [batch_size, num_attrs]\n",
      "\n",
      "        # Handle hair color (4 classes)\n",
      "        hair_mask = is_hair_color\n",
      "        if hair_mask.any():\n",
      "            # For hair color, randomly select a different class\n",
      "            new_hair_vals = torch.randint(0, 4, (hair_mask.sum(),), device=attrs.device).float()\n",
      "            # Make sure new values are different from original\n",
      "            same_hair = new_hair_vals == orig_attr_vals[hair_mask]\n",
      "            if same_hair.any():\n",
      "                new_hair_vals[same_hair] = (new_hair_vals[same_hair] + 1) % 4\n",
      "            perturbed_attrs[hair_mask, 0] = new_hair_vals\n",
      "\n",
      "        # Handle binary attributes\n",
      "        binary_mask = ~is_hair_color\n",
      "        if binary_mask.any():\n",
      "            # For binary attributes, flip the value\n",
      "            binary_indices = attr_indices[binary_mask]\n",
      "            perturbed_attrs[binary_mask, binary_indices] = 1 - orig_attr_vals[binary_mask]\n",
      "\n",
      "        # Get reconstructions for entire batch\n",
      "        with torch.no_grad():\n",
      "            # Original reconstructions\n",
      "            orig_decoded = vae.decoder(torch.cat([z, attrs], dim=1))\n",
      "\n",
      "            # Perturbed reconstructions\n",
      "            perturbed_decoded = vae.decoder(torch.cat([z, perturbed_attrs], dim=1))\n",
      "\n",
      "            # Get CLIP embeddings for all images\n",
      "            orig_features = self.get_image_features(orig_decoded)\n",
      "            perturbed_features = self.get_image_features(perturbed_decoded)\n",
      "\n",
      "            # Initialize lists for text features\n",
      "            all_orig_text_features = []\n",
      "            all_perturbed_text_features = []\n",
      "\n",
      "            # Get text features for each attribute type\n",
      "            attr_names = ['hair_color', 'pale_skin', 'gender', 'beard']\n",
      "            for attr_type in range(4):\n",
      "                mask = (attr_indices == attr_type)\n",
      "                if not mask.any():\n",
      "                    continue\n",
      "\n",
      "                # Get original descriptions\n",
      "                orig_descriptions = []\n",
      "                for idx in range(batch_size):\n",
      "                    if mask[idx]:\n",
      "                        orig_descriptions.extend(\n",
      "                            self.attribute_templates[attr_names[attr_type]][int(orig_attr_vals[idx].item())]\n",
      "                        )\n",
      "\n",
      "                # Get perturbed descriptions\n",
      "                perturbed_descriptions = []\n",
      "                for idx in range(batch_size):\n",
      "                    if mask[idx]:\n",
      "                        perturbed_descriptions.extend(\n",
      "                            self.attribute_templates[attr_names[attr_type]][int(perturbed_attrs[idx, attr_type].item())]\n",
      "                        )\n",
      "\n",
      "                if orig_descriptions:  # If we have any descriptions for this attribute\n",
      "                    orig_text_features = self.get_text_features(orig_descriptions)\n",
      "                    perturbed_text_features = self.get_text_features(perturbed_descriptions)\n",
      "\n",
      "                    # Reshape to account for multiple descriptions per image\n",
      "                    num_desc = len(self.attribute_templates[attr_names[attr_type]][0])\n",
      "                    orig_text_features = orig_text_features.view(-1, num_desc, orig_text_features.size(-1))\n",
      "                    perturbed_text_features = perturbed_text_features.view(-1, num_desc,\n",
      "                                                                           perturbed_text_features.size(-1))\n",
      "\n",
      "                    all_orig_text_features.append((mask, orig_text_features))\n",
      "                    all_perturbed_text_features.append((mask, perturbed_text_features))\n",
      "\n",
      "            # Compute losses for each attribute type and combine\n",
      "            total_loss = 0\n",
      "            margin = 0.2\n",
      "\n",
      "            for (mask, orig_text), (_, perturbed_text) in zip(all_orig_text_features, all_perturbed_text_features):\n",
      "                if not mask.any():\n",
      "                    continue\n",
      "\n",
      "                # Compute similarities for masked batch\n",
      "                orig_features_masked = orig_features[mask]\n",
      "                perturbed_features_masked = perturbed_features[mask]\n",
      "\n",
      "                # Compute mean similarity across multiple descriptions\n",
      "                orig_sim = (orig_features_masked @ orig_text.mean(dim=1).t()).mean(dim=1)\n",
      "                perturbed_sim = (perturbed_features_masked @ perturbed_text.mean(dim=1).t()).mean(dim=1)\n",
      "\n",
      "                # Cross similarities\n",
      "                cross_orig_sim = (orig_features_masked @ perturbed_text.mean(dim=1).t()).mean(dim=1)\n",
      "                cross_perturbed_sim = (perturbed_features_masked @ orig_text.mean(dim=1).t()).mean(dim=1)\n",
      "\n",
      "                # Compute contrastive loss\n",
      "                loss = (\n",
      "                        torch.relu(cross_orig_sim - orig_sim + margin) +\n",
      "                        torch.relu(cross_perturbed_sim - perturbed_sim + margin)\n",
      "                ).mean()\n",
      "\n",
      "                total_loss += loss\n",
      "\n",
      "            return total_loss / len(all_orig_text_features)\n",
      "\n",
      "----------------------------------------\n",
      "File: celeba_project.py\n",
      "\n",
      "from PIL import Image\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import matplotlib.pyplot as plt\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "import torchvision.transforms as transforms\n",
      "import os\n",
      "from PIL import Image\n",
      "\n",
      "import torch.optim as  optim\n",
      "from tqdm import tqdm\n",
      "\n",
      "from clip_loss import CLIPAttributeConsistency\n",
      "from dataset import CelebaDataset, transform\n",
      "from models import Encoder, Decoder, CVAE, loss_function, CosineScheduler\n",
      "from plots import plot_reconstructions, save_training_visualizations\n",
      "\n",
      "if torch.cuda.is_available():\n",
      "  dev = \"cuda:0\"\n",
      "  print(\"gpu up\")\n",
      "else:\n",
      "  dev = \"cpu\"\n",
      "device = torch.device(dev)\n",
      "\n",
      "df = pd.read_csv(\"celeba_dataset/list_attr_celeba.csv\")\n",
      "\n",
      "def haircolor(x):\n",
      "    if x[\"Blond_Hair\"] == 1:\n",
      "        return 0\n",
      "    elif x[\"Brown_Hair\"] == 1:\n",
      "        return 1\n",
      "    elif x[\"Black_Hair\"] == 1:\n",
      "        return 2\n",
      "    else:\n",
      "        return 3\n",
      "\n",
      "\n",
      "df[\"Hair_Color\"] = df.apply(haircolor, axis=1)\n",
      "\n",
      "df = df[[\"image_id\",\"Hair_Color\",'Pale_Skin',\"Male\",\"No_Beard\"]]\n",
      "df.Pale_Skin = df.Pale_Skin.apply(lambda x: max(x,0))\n",
      "df.Male = df.Male.apply(lambda x: max(x,0))\n",
      "df.No_Beard = df.No_Beard.apply(lambda x: max(x,0))\n",
      "\n",
      "\n",
      "\n",
      "# Initialize dataset and dataloader\n",
      "dataset = CelebaDataset(\n",
      "    df=df,\n",
      "    img_dir = \"/home/omrid/Desktop/jungo /projectCLIPvae/celeba_dataset/img_align_celeba/img_align_celeba/\",\n",
      "    transform=transform\n",
      ")\n",
      "\n",
      "dataloader = DataLoader(\n",
      "    dataset,\n",
      "    batch_size=128,\n",
      "    shuffle=True,\n",
      "    num_workers=4,  # Parallel data loading\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "vae = CVAE(Encoder, Decoder)\n",
      "vae.to(device)\n",
      "clip_consistency = CLIPAttributeConsistency(device=device)\n",
      "\n",
      "\n",
      "def train_vae(vae, dataloader, optimizer, device, num_epochs=1201, save_interval=100):\n",
      "    \"\"\"\n",
      "    Train the VAE model using BCE loss with beta scheduling\n",
      "    \"\"\"\n",
      "    vae.train()\n",
      "    loss_history = {\n",
      "        'total': [],\n",
      "        'reconstruction': [],\n",
      "        'kl': [],\n",
      "        'beta': []\n",
      "    }\n",
      "\n",
      "    # Initialize beta scheduler\n",
      "    beta_scheduler = CosineScheduler(\n",
      "        start_value=0.0,  # Start with low KL weight\n",
      "        end_value=1.0,  # End with full KL weight\n",
      "        num_cycles=4,  # Number of cycles during training\n",
      "        num_epochs=num_epochs\n",
      "    )\n",
      "\n",
      "    for epoch in range(num_epochs):\n",
      "        pbar = tqdm(total=len(dataloader), desc=f'Epoch {epoch}')\n",
      "        epoch_total_loss = 0\n",
      "        epoch_recon_loss = 0\n",
      "        epoch_kl_loss = 0\n",
      "\n",
      "        # Get current beta value\n",
      "        current_beta = beta_scheduler.get_value(epoch)\n",
      "\n",
      "        # Store first batch for visualization\n",
      "        vis_batch = None\n",
      "\n",
      "        for batch_idx, (images, attrs) in enumerate(dataloader):\n",
      "            if batch_idx == 0:\n",
      "                vis_batch = (images[:5].clone(), attrs[:5].clone())\n",
      "\n",
      "            images = images.to(device)\n",
      "            attrs = attrs.to(device)\n",
      "\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            encoder_output = vae.encoder(images)\n",
      "            recon_images, mu, logvar = vae(images, attrs)\n",
      "\n",
      "            # Compute losses with current beta value\n",
      "            total_loss, recon_loss, kl_loss, clip_loss = loss_function(\n",
      "                recon_images,\n",
      "                images,\n",
      "                mu,\n",
      "                logvar,\n",
      "                vae,\n",
      "                encoder_output,\n",
      "                attrs,\n",
      "                beta_vae=1.0,\n",
      "                beta_clip=0.2,\n",
      "                clip_consistency=clip_consistency  # Pass the initialized checker\n",
      "            )\n",
      "\n",
      "            total_loss.backward()\n",
      "            optimizer.step()\n",
      "\n",
      "            # Accumulate losses\n",
      "            epoch_total_loss += total_loss.item()\n",
      "            epoch_recon_loss += recon_loss.item()\n",
      "            epoch_kl_loss += kl_loss.item()\n",
      "\n",
      "            pbar.update(1)\n",
      "\n",
      "        pbar.close()\n",
      "\n",
      "        # Calculate averages\n",
      "        avg_total = epoch_total_loss / len(dataloader)\n",
      "        avg_recon = epoch_recon_loss / len(dataloader)\n",
      "        avg_kl = epoch_kl_loss / len(dataloader)\n",
      "\n",
      "        # Store in history\n",
      "        loss_history['total'].append(avg_total)\n",
      "        loss_history['reconstruction'].append(avg_recon)\n",
      "        loss_history['kl'].append(avg_kl)\n",
      "        loss_history['beta'].append(current_beta)\n",
      "\n",
      "        print(f\"\\nEpoch {epoch} Summary:\")\n",
      "        print(f\"Total Loss: {avg_total:.6f}\")\n",
      "        print(f\"BCE Loss: {avg_recon:.6f}\")\n",
      "        print(f\"KL Loss: {avg_kl:.6f}\")\n",
      "        print(f\"Current Beta: {current_beta:.6f}\\n\")\n",
      "\n",
      "        # Generate and save reconstructions\n",
      "        if vis_batch is not None:\n",
      "            save_training_visualizations(vae, clip_consistency, vis_batch, epoch)\n",
      "\n",
      "        # Save checkpoint at intervals\n",
      "        if epoch % save_interval == 0:\n",
      "            torch.save({\n",
      "                'epoch': epoch,\n",
      "                'model_state_dict': vae.state_dict(),\n",
      "                'optimizer_state_dict': optimizer.state_dict(),\n",
      "                'loss': avg_total,\n",
      "                'beta': current_beta\n",
      "            }, f\"vae_checkpoint_epoch_{epoch}.pt\")\n",
      "\n",
      "    return loss_history\n",
      "\n",
      "\n",
      "# Setup training\n",
      "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "vae = vae.to(device)\n",
      "optimizer =  optim.Adam(vae.parameters(), lr=0.0001)  # Adjust learning rate if needed\n",
      "\n",
      "# Train the model\n",
      "loss_history = train_vae(\n",
      "    vae=vae,\n",
      "    dataloader=dataloader,\n",
      "    optimizer=optimizer,\n",
      "    device=device,\n",
      "    num_epochs=1201,\n",
      "    save_interval=100\n",
      ")\n",
      "# Plot training progress\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(loss_history)\n",
      "plt.title('Training Loss Over Time')\n",
      "plt.xlabel('Epoch')\n",
      "plt.ylabel('Loss')\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "File: dataset.py\n",
      "\n",
      "import os\n",
      "import torch\n",
      "from PIL import Image\n",
      "from torch.utils.data import Dataset\n",
      "import torchvision.transforms as transforms\n",
      "\n",
      "\n",
      "class CelebaDataset(Dataset):\n",
      "    def __init__(self, df, img_dir, transform=None, attribute_names=None):\n",
      "        \"\"\"\n",
      "        Initialize dataset with flexible attributes\n",
      "\n",
      "        Args:\n",
      "            df: DataFrame containing image_id and attribute columns\n",
      "            img_dir: Directory containing images\n",
      "            transform: Image transforms\n",
      "            attribute_names: List of attribute names to use\n",
      "        \"\"\"\n",
      "        self.df = df\n",
      "        self.img_dir = img_dir\n",
      "        self.transform = transform\n",
      "        self.attribute_names = attribute_names or [col for col in df.columns if col != 'image_id']\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.df)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        img_name = self.df.iloc[idx]['image_id']\n",
      "        img_path = os.path.join(self.img_dir, img_name)\n",
      "        image = Image.open(img_path).convert('RGB')\n",
      "\n",
      "        if self.transform:\n",
      "            image = self.transform(image)\n",
      "\n",
      "        # Extract only specified attributes\n",
      "        attrs = self.df.iloc[idx][self.attribute_names].values\n",
      "        attrs = torch.tensor(attrs.astype('float32'))\n",
      "\n",
      "        return image, attrs\n",
      "\n",
      "\n",
      "# Define transforms\n",
      "transform = transforms.Compose([\n",
      "    transforms.Resize((64, 64)),\n",
      "    transforms.ToTensor(),\n",
      "])\n",
      "----------------------------------------\n",
      "File: clip_classifier.py\n",
      "\n",
      "import torch\n",
      "import clip\n",
      "from PIL import Image\n",
      "import pandas as pd\n",
      "from dotenv import load_dotenv\n",
      "from tqdm import tqdm\n",
      "import os\n",
      "from typing import Dict, List, Union\n",
      "import torch.nn.functional as F\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
      "import json\n",
      "import dotenv\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "\n",
      "class TemplateGenerator:\n",
      "    def __init__(self, api_key: str):\n",
      "        \"\"\"Initialize the template generator with OpenAI API key\"\"\"\n",
      "        print(f\"\\nInitializing Template Generator...\")\n",
      "        print(f\"API Key provided: {'Yes' if api_key else 'No'}\")\n",
      "\n",
      "        try:\n",
      "            self.llm = ChatOpenAI(\n",
      "                model_name=\"gpt-3.5-turbo-0125\",\n",
      "                temperature=0.7,\n",
      "                openai_api_key=api_key\n",
      "            )\n",
      "            print(\"Successfully initialized ChatOpenAI\")\n",
      "        except Exception as e:\n",
      "            print(f\"Error initializing ChatOpenAI: {str(e)}\")\n",
      "            raise\n",
      "\n",
      "        # Create a clean, single-line prompt template\n",
      "        template = (\n",
      "            \"Generate natural language templates that describe a person's appearance for CLIP image classification.\\n\\n\"\n",
      "            \"IMPORTANT INSTRUCTIONS:\\n\"\n",
      "            \"- Each template should be a simple, clear sentence focusing ONLY on the specific attribute value\\n\"\n",
      "            \"- DO NOT add any assumptions or extra information beyond the attribute value\\n\"\n",
      "            \"- DO NOT specify age ranges, specific details, or additional characteristics\\n\"\n",
      "            \"- DO NOT use subjective or interpretive descriptions\\n\"\n",
      "            \"- FOCUS on distinguishing between the categories within each attribute\\n\"\n",
      "            \"- Keep descriptions general and factual\\n\"\n",
      "            \"- MAKE SURE to use the exact attribute values provided, without modifications\\n\\n\"\n",
      "            \"For example, if given:\\n\"\n",
      "            \"ethnicity: African, Asian, European\\n\\n\"\n",
      "            \"Expected Output:\\n\"\n",
      "            '{{\\n'\n",
      "            '    \"ethnicity\": {{\\n'\n",
      "            '        \"African\": [\\n'\n",
      "            '            \"a photograph of a person with African features\",\\n'\n",
      "            '            \"an African person\",\\n'\n",
      "            '            \"a face with African characteristics\",\\n'\n",
      "            '            \"a portrait of an African person\",\\n'\n",
      "            '            \"a person of African descent\"\\n'\n",
      "            '        ],\\n'\n",
      "            '        \"Asian\": [\\n'\n",
      "            '            \"a photograph of a person with Asian features\",\\n'\n",
      "            '            \"an Asian person\",\\n'\n",
      "            '            \"a face with Asian characteristics\",\\n'\n",
      "            '            \"a portrait of an Asian person\",\\n'\n",
      "            '            \"a person of Asian descent\"\\n'\n",
      "            '        ],\\n'\n",
      "            '        \"European\": [\\n'\n",
      "            '            \"a photograph of a person with European features\",\\n'\n",
      "            '            \"a European person\",\\n'\n",
      "            '            \"a face with European characteristics\",\\n'\n",
      "            '            \"a portrait of a European person\",\\n'\n",
      "            '            \"a person of European descent\"\\n'\n",
      "            '        ]\\n'\n",
      "            '    }}\\n'\n",
      "            '}}\\n\\n'\n",
      "            \"Now, please generate templates for the following attributes and values:\\n\"\n",
      "            \"{attributes}\\n\\n\"\n",
      "            \"The output should be in the same JSON format as the example.\\n\"\n",
      "            \"Remember to keep descriptions simple and focused only on the specific attribute value.\\n\"\n",
      "            \"Only include the JSON output, no additional text.\"\n",
      "        )\n",
      "\n",
      "        self.prompt = PromptTemplate(\n",
      "            template=template,\n",
      "            input_variables=[\"attributes\"]\n",
      "        )\n",
      "\n",
      "    def generate_templates(self, attribute_values: Dict[str, List[str]]) -> Dict[str, Dict[str, List[str]]]:\n",
      "        \"\"\"Generate templates using GPT for given attributes and values\"\"\"\n",
      "        try:\n",
      "            # Format attributes for the prompt\n",
      "            attributes_str = \"\\n\".join(\n",
      "                f\"{attr}: {', '.join(values)}\"\n",
      "                for attr, values in attribute_values.items()\n",
      "            )\n",
      "\n",
      "            print(\"\\nGenerating templates for attributes:\")\n",
      "            print(attributes_str)\n",
      "\n",
      "            # Generate the prompt using the template\n",
      "            prompt = self.prompt.format(attributes=attributes_str)\n",
      "\n",
      "            print(\"\\nSending request to GPT...\")\n",
      "            # Get response from GPT\n",
      "            response = self.llm.predict(prompt)\n",
      "\n",
      "            print(\"\\nReceived response from GPT. First 200 characters:\")\n",
      "            print(response[:200] + \"...\" if len(response) > 200 else response)\n",
      "\n",
      "            # Parse the JSON response\n",
      "            try:\n",
      "                templates = json.loads(response)\n",
      "                print(\"\\nSuccessfully parsed JSON response\")\n",
      "                # Debug: Print template structure\n",
      "                print(\"\\nTemplate structure:\")\n",
      "                for attr, values in templates.items():\n",
      "                    print(f\"\\n{attr}:\")\n",
      "                    for val, temps in values.items():\n",
      "                        print(f\"  {val}: {len(temps)} templates\")\n",
      "                return templates\n",
      "            except json.JSONDecodeError as e:\n",
      "                print(f\"\\nError parsing GPT response as JSON: {str(e)}\")\n",
      "                print(\"Raw response:\", response)\n",
      "                return None\n",
      "\n",
      "        except Exception as e:\n",
      "            print(f\"\\nError in template generation: {str(e)}\")\n",
      "            import traceback\n",
      "            traceback.print_exc()\n",
      "            return None\n",
      "\n",
      "\n",
      "class CustomAttributeClassifier:\n",
      "    def __init__(self,\n",
      "                 clip_model_name: str = \"ViT-B/32\",\n",
      "                 device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
      "                 openai_api_key: str = None,\n",
      "                 debug: bool = False):\n",
      "        self.device = device\n",
      "        self.debug = debug\n",
      "        print(f\"Initializing CLIP model on {device}\")\n",
      "\n",
      "        # First try to get the API key from the parameter, then from environment\n",
      "        self.api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "        if self.api_key:\n",
      "            print(\"OpenAI API key found!\")\n",
      "            self.template_generator = TemplateGenerator(self.api_key)\n",
      "        else:\n",
      "            print(\"No OpenAI API key found in parameters or environment variables.\")\n",
      "            self.template_generator = None\n",
      "\n",
      "        self.model, self.preprocess = clip.load(clip_model_name, device=device)\n",
      "        self.templates = None\n",
      "        self.attribute_mappings = {}\n",
      "\n",
      "    def normalize_templates(self, gpt_templates: Dict) -> Dict:\n",
      "        \"\"\"\n",
      "        Normalize templates from GPT format to our indexed format\n",
      "        \"\"\"\n",
      "        normalized = {}\n",
      "        for attr, values in gpt_templates.items():\n",
      "            normalized[attr] = {}\n",
      "            # Create a mapping from value to index\n",
      "            value_to_idx = {value: idx for idx, value in self.attribute_mappings[attr].items()}\n",
      "\n",
      "            # Convert string keys to numeric indices\n",
      "            for value, templates in values.items():\n",
      "                if value in value_to_idx:\n",
      "                    idx = value_to_idx[value]\n",
      "                    normalized[attr][idx] = templates\n",
      "                else:\n",
      "                    print(f\"Warning: Value '{value}' not found in mappings for {attr}\")\n",
      "\n",
      "        return normalized\n",
      "\n",
      "    def print_templates(self):\n",
      "        \"\"\"Print the full template structure in a clear, organized format\"\"\"\n",
      "        print(\"\\nCurrent Templates:\")\n",
      "        print(\"=\" * 50)\n",
      "\n",
      "        for attr, templates in self.templates.items():\n",
      "            print(f\"\\n{attr.upper()}:\")\n",
      "            print(\"-\" * 30)\n",
      "\n",
      "            # Sort by index to maintain consistent order\n",
      "            for idx in sorted(templates.keys()):\n",
      "                value = self.attribute_mappings[attr][idx]\n",
      "                print(f\"\\n{value}:\")\n",
      "                for i, template in enumerate(templates[idx], 1):\n",
      "                    print(f\"  {i}. {template}\")\n",
      "\n",
      "        print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
      "\n",
      "    def create_text_templates(self, attribute_values: Dict[str, List[str]]) -> Dict:\n",
      "        \"\"\"Create text templates using GPT if available, otherwise use default templates\"\"\"\n",
      "        # First create attribute mappings\n",
      "        self.attribute_mappings = {\n",
      "            attr: {idx: value for idx, value in enumerate(values)}\n",
      "            for attr, values in attribute_values.items()\n",
      "        }\n",
      "\n",
      "        if self.template_generator:\n",
      "            print(\"\\nAttempting to generate templates using GPT...\")\n",
      "            try:\n",
      "                gpt_templates = self.template_generator.generate_templates(attribute_values)\n",
      "                if gpt_templates:\n",
      "                    print(\"Successfully generated templates using GPT!\")\n",
      "                    # Normalize the templates to use indices\n",
      "                    self.templates = self.normalize_templates(gpt_templates)\n",
      "                    return self.templates\n",
      "\n",
      "            except Exception as e:\n",
      "                print(f\"\\nError during GPT template generation: {str(e)}\")\n",
      "                print(\"Falling back to custom templates.\")\n",
      "        else:\n",
      "            print(\"\\nGPT template generator not available (no API key provided).\")\n",
      "            print(\"Using custom templates instead.\")\n",
      "\n",
      "        # Fallback to default templates\n",
      "        templates = {}\n",
      "        for attr, values in attribute_values.items():\n",
      "            templates[attr] = {}\n",
      "            for idx, value in enumerate(values):\n",
      "                templates[attr][idx] = [\n",
      "                    f\"a photo of a person with {value} appearance\",\n",
      "                    f\"a {value} person\",\n",
      "                    f\"a face with {value} features\",\n",
      "                    f\"this person appears to be {value}\",\n",
      "                    f\"a portrait showing {value} characteristics\"\n",
      "                ]\n",
      "\n",
      "        self.templates = templates\n",
      "        return templates\n",
      "\n",
      "    def classify_images(self,\n",
      "                       img_dir: str,\n",
      "                       attribute_values: Dict[str, List[str]],\n",
      "                       batch_size: int = 8) -> pd.DataFrame:\n",
      "        \"\"\"Classify images according to specified attributes using CLIP\"\"\"\n",
      "        if not os.path.exists(img_dir):\n",
      "            raise ValueError(f\"Directory not found: {img_dir}\")\n",
      "\n",
      "        # Create or get templates\n",
      "        if not self.templates:\n",
      "            print(\"Generating templates...\")\n",
      "            self.create_text_templates(attribute_values)\n",
      "            # Print templates once at initialization\n",
      "            self.print_templates()\n",
      "\n",
      "        print(f\"\\nScanning directory: {img_dir}\")\n",
      "        image_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
      "\n",
      "        if not image_files:\n",
      "            raise ValueError(f\"No image files found in {img_dir}\")\n",
      "\n",
      "        print(f\"Found {len(image_files)} images\")\n",
      "\n",
      "        results = {'image_id': []}\n",
      "        for attr in attribute_values.keys():\n",
      "            results[attr] = []\n",
      "\n",
      "        # Process images in batches\n",
      "        for i in tqdm(range(0, len(image_files), batch_size),\n",
      "                     desc=\"Classifying images\",\n",
      "                     position=0,\n",
      "                     leave=True):\n",
      "            batch_files = image_files[i:i + batch_size]\n",
      "            batch_images = []\n",
      "            valid_files = []\n",
      "\n",
      "            # Load and preprocess images\n",
      "            for img_file in batch_files:\n",
      "                try:\n",
      "                    img_path = os.path.join(img_dir, img_file)\n",
      "                    image = Image.open(img_path).convert('RGB')\n",
      "                    processed_image = self.preprocess(image)\n",
      "                    batch_images.append(processed_image)\n",
      "                    valid_files.append(img_file)\n",
      "                except Exception as e:\n",
      "                    if self.debug:\n",
      "                        print(f\"Error processing {img_file}: {str(e)}\")\n",
      "                    continue\n",
      "\n",
      "            if not batch_images:\n",
      "                continue\n",
      "\n",
      "            try:\n",
      "                image_batch = torch.stack(batch_images).to(self.device)\n",
      "\n",
      "                with torch.no_grad():\n",
      "                    image_features = self.model.encode_image(image_batch)\n",
      "                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
      "\n",
      "                    for attr, values in attribute_values.items():\n",
      "                        try:\n",
      "                            if self.debug:\n",
      "                                print(f\"\\nProcessing attribute: {attr}\")\n",
      "                                print(f\"Values: {values}\")\n",
      "\n",
      "                            all_text_features = []\n",
      "                            templates = self.templates[attr]\n",
      "\n",
      "                            for idx, value in enumerate(values):\n",
      "                                try:\n",
      "                                    # Try both integer and string keys\n",
      "                                    if idx in templates:\n",
      "                                        key = idx\n",
      "                                    elif str(idx) in templates:\n",
      "                                        key = str(idx)\n",
      "                                    else:\n",
      "                                        raise KeyError(f\"Neither key {idx} nor '{idx}' found in templates\")\n",
      "\n",
      "                                    texts = templates[key]\n",
      "                                    text_tokens = clip.tokenize(texts).to(self.device)\n",
      "                                    text_features = self.model.encode_text(text_tokens)\n",
      "                                    text_features = text_features.mean(dim=0, keepdim=True)\n",
      "                                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
      "                                    all_text_features.append(text_features)\n",
      "\n",
      "                                except Exception as e:\n",
      "                                    if self.debug:\n",
      "                                        print(f\"Error processing value {value}:\")\n",
      "                                        print(f\"Full error: {str(e)}\")\n",
      "                                        import traceback\n",
      "                                        traceback.print_exc()\n",
      "                                    raise\n",
      "\n",
      "                            text_features = torch.cat(all_text_features)\n",
      "                            similarity = (100.0 * image_features @ text_features.T)\n",
      "                            predictions = similarity.softmax(dim=-1).cpu().numpy()\n",
      "                            class_indices = predictions.argmax(axis=1)\n",
      "\n",
      "                            results[attr].extend(class_indices.tolist())\n",
      "\n",
      "                        except Exception as e:\n",
      "                            if self.debug:\n",
      "                                print(f\"Error processing attribute {attr}:\")\n",
      "                                print(f\"Full error: {str(e)}\")\n",
      "                                import traceback\n",
      "                                traceback.print_exc()\n",
      "                            raise\n",
      "\n",
      "                results['image_id'].extend(valid_files)\n",
      "\n",
      "            except Exception as e:\n",
      "                if self.debug:\n",
      "                    print(f\"Error processing batch:\")\n",
      "                    print(f\"Full error: {str(e)}\")\n",
      "                    import traceback\n",
      "                    traceback.print_exc()\n",
      "                continue\n",
      "\n",
      "        df = pd.DataFrame(results)\n",
      "        self._print_distribution_statistics(df)\n",
      "        return df\n",
      "\n",
      "    def _print_distribution_statistics(self, df: pd.DataFrame):\n",
      "        \"\"\"Print distribution statistics for each attribute\"\"\"\n",
      "        print(f\"\\nProcessed {len(df)} images successfully\")\n",
      "        print(\"\\nAttribute distribution:\")\n",
      "\n",
      "        for attr, mapping in self.attribute_mappings.items():\n",
      "            print(f\"\\n{attr.capitalize()} Distribution:\")\n",
      "            print(\"-\" * 30)\n",
      "\n",
      "            counts = df[attr].value_counts().sort_index()\n",
      "            total = len(df)\n",
      "\n",
      "            for idx in sorted(mapping.keys()):\n",
      "                count = counts.get(idx, 0)\n",
      "                percentage = (count / total) * 100\n",
      "                print(f\"{mapping[idx]:<25}: {count:>6,} ({percentage:>6.1f}%)\")\n",
      "\n",
      "\n",
      "def prepare_attributes_for_vae(df: pd.DataFrame,\n",
      "                               attribute_mapping: Dict[str, List[str]]) -> pd.DataFrame:\n",
      "    \"\"\"Prepare the classified attributes for use in the VAE\"\"\"\n",
      "    vae_df = df.copy()\n",
      "\n",
      "    for attr, values in attribute_mapping.items():\n",
      "        vae_df[attr] = vae_df[attr].astype(int)\n",
      "\n",
      "    return vae_df\n",
      "----------------------------------------\n",
      "File: plots.py\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "\n",
      "def plot_reconstructions(original_images, reconstructed_images, attrs, epoch):\n",
      "    \"\"\"\n",
      "    Plot 5 original and reconstructed images side by side with their attributes\n",
      "    \"\"\"\n",
      "    fig, axes = plt.subplots(5, 2, figsize=(12, 15))\n",
      "    fig.suptitle(f'Original vs Reconstructed Images - Epoch {epoch}')\n",
      "\n",
      "    # Labels for attributes\n",
      "    hair_labels = ['Blond', 'Brown', 'Black', 'Other']\n",
      "\n",
      "    for idx in range(5):\n",
      "        if idx < len(original_images):\n",
      "            # Original image\n",
      "            orig_img = original_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 0].imshow(orig_img)\n",
      "            axes[idx, 0].set_title('Original')\n",
      "\n",
      "            # Reconstructed image\n",
      "            recon_img = reconstructed_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 1].imshow(recon_img)\n",
      "            axes[idx, 1].set_title('Reconstructed')\n",
      "\n",
      "            # Get attributes for this image\n",
      "            attr = attrs[idx]\n",
      "            attr_text = f'Hair: {hair_labels[int(attr[0])]} | '\n",
      "            attr_text += f'Pale Skin: {bool(attr[1])} | '\n",
      "            attr_text += f'Male: {bool(attr[2])} | '\n",
      "            attr_text += f'No Beard: {bool(attr[3])}'\n",
      "\n",
      "            # Add attributes as text below the images\n",
      "            plt.figtext(0.5, 0.98 - (idx * 0.2), attr_text,\n",
      "                        ha='center', va='top', bbox=dict(facecolor='white', alpha=0.8))\n",
      "\n",
      "            # Remove axes\n",
      "            axes[idx, 0].axis('off')\n",
      "            axes[idx, 1].axis('off')\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(f'/home/omrid/Desktop/jungo /projectCLIPvae/plot_reconstructions/reconstructions_epoch_{epoch}.png')\n",
      "    plt.close()\n",
      "\n",
      "\n",
      "def plot_training_progress(loss_history):\n",
      "    \"\"\"\n",
      "    Plot training progress including losses and beta values\n",
      "    \"\"\"\n",
      "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
      "\n",
      "    # Plot losses\n",
      "    ax1.plot(loss_history['total'], label='Total Loss')\n",
      "    ax1.plot(loss_history['reconstruction'], label='Reconstruction Loss')\n",
      "    ax1.plot(loss_history['kl'], label='KL Loss')\n",
      "    ax1.set_title('Training Losses Over Time')\n",
      "    ax1.set_xlabel('Epoch')\n",
      "    ax1.set_ylabel('Loss')\n",
      "    ax1.grid(True)\n",
      "    ax1.legend()\n",
      "\n",
      "    # Plot beta values\n",
      "    ax2.plot(loss_history['beta'], label='Beta Value', color='purple')\n",
      "    ax2.set_title('Beta Schedule Over Time')\n",
      "    ax2.set_xlabel('Epoch')\n",
      "    ax2.set_ylabel('Beta')\n",
      "    ax2.grid(True)\n",
      "    ax2.legend()\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import torch\n",
      "\n",
      "\n",
      "def plot_reconstructions_with_perturbations(original_images, reconstructed_images,\n",
      "                                            perturbed_images, orig_attrs, perturbed_attrs,\n",
      "                                            attr_indices, epoch):\n",
      "    \"\"\"\n",
      "    Plot original, reconstructed, and perturbed images side by side\n",
      "\n",
      "    Args:\n",
      "        original_images: Original input images\n",
      "        reconstructed_images: VAE reconstructed images\n",
      "        perturbed_images: Images with perturbed attributes\n",
      "        orig_attrs: Original attributes\n",
      "        perturbed_attrs: Perturbed attributes\n",
      "        attr_indices: Which attribute was modified for each image\n",
      "        epoch: Current training epoch\n",
      "    \"\"\"\n",
      "    num_samples = min(5, len(original_images))\n",
      "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 4 * num_samples))\n",
      "    fig.suptitle(f'Original vs Reconstructed vs Perturbed Images - Epoch {epoch}')\n",
      "\n",
      "    # Labels for attributes\n",
      "    attr_names = ['Hair Color', 'Pale Skin', 'Gender', 'No Beard']\n",
      "    hair_labels = ['Blonde', 'Brown', 'Black', 'Other']\n",
      "    binary_labels = {\n",
      "        'Pale Skin': ['Dark', 'Pale'],\n",
      "        'Gender': ['Female', 'Male'],\n",
      "        'No Beard': ['Has Beard', 'No Beard']\n",
      "    }\n",
      "\n",
      "    for idx in range(num_samples):\n",
      "        if idx < len(original_images):\n",
      "            # Original image\n",
      "            orig_img = original_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 0].imshow(orig_img)\n",
      "            axes[idx, 0].set_title('Original')\n",
      "\n",
      "            # Reconstructed image\n",
      "            recon_img = reconstructed_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 1].imshow(recon_img)\n",
      "            axes[idx, 1].set_title('Reconstructed')\n",
      "\n",
      "            # Perturbed image\n",
      "            perturbed_img = perturbed_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 2].imshow(perturbed_img)\n",
      "\n",
      "            # Get perturbed attribute info\n",
      "            attr_idx = attr_indices[idx].item()\n",
      "            attr_name = attr_names[attr_idx]\n",
      "\n",
      "            if attr_idx == 0:  # Hair color\n",
      "                orig_val = hair_labels[int(orig_attrs[idx, attr_idx].item())]\n",
      "                new_val = hair_labels[int(perturbed_attrs[idx, attr_idx].item())]\n",
      "            else:  # Binary attributes\n",
      "                orig_val = binary_labels[attr_name][int(orig_attrs[idx, attr_idx].item())]\n",
      "                new_val = binary_labels[attr_name][int(perturbed_attrs[idx, attr_idx].item())]\n",
      "\n",
      "            axes[idx, 2].set_title(f'Perturbed: {attr_name}\\n{orig_val}  {new_val}')\n",
      "\n",
      "            # Remove axes\n",
      "            for ax in axes[idx]:\n",
      "                ax.axis('off')\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(os.path.join(save_dir, f'visualizations_epoch_{epoch}.png'))\n",
      "    plt.close()\n",
      "\n",
      "def generate_perturbation_samples(vae, clip_consistency, images, attrs, device):\n",
      "    \"\"\"\n",
      "    Generate samples with perturbed attributes for visualization\n",
      "    \"\"\"\n",
      "    vae.eval()\n",
      "    with torch.no_grad():\n",
      "        # Get encoder output\n",
      "        encoder_output = vae.encoder(images)\n",
      "        z = encoder_output[:, :64]\n",
      "\n",
      "        # Get original reconstructions\n",
      "        recon_images = vae.decoder(torch.cat([z, attrs], dim=1))\n",
      "\n",
      "        # Create perturbed attributes\n",
      "        batch_size = attrs.size(0)\n",
      "        attr_indices = torch.randint(0, 4, (batch_size,), device=device)\n",
      "\n",
      "        # Create perturbed attributes following same logic as in clip loss\n",
      "        perturbed_attrs = attrs.clone().float()\n",
      "\n",
      "        # Handle hair color (4 classes)\n",
      "        hair_mask = (attr_indices == 0)\n",
      "        if hair_mask.any():\n",
      "            new_hair_vals = torch.randint(0, 4, (hair_mask.sum(),), device=device).float()\n",
      "            same_hair = new_hair_vals == attrs[hair_mask, 0]\n",
      "            if same_hair.any():\n",
      "                new_hair_vals[same_hair] = (new_hair_vals[same_hair] + 1) % 4\n",
      "            perturbed_attrs[hair_mask, 0] = new_hair_vals\n",
      "\n",
      "        # Handle binary attributes\n",
      "        binary_mask = ~hair_mask\n",
      "        if binary_mask.any():\n",
      "            binary_indices = attr_indices[binary_mask]\n",
      "            orig_vals = torch.gather(attrs[binary_mask], 1, binary_indices.unsqueeze(1)).squeeze(1)\n",
      "            perturbed_attrs[binary_mask, binary_indices] = 1 - orig_vals\n",
      "\n",
      "        # Generate perturbed reconstructions\n",
      "        perturbed_images = vae.decoder(torch.cat([z, perturbed_attrs], dim=1))\n",
      "\n",
      "        return recon_images, perturbed_images, perturbed_attrs, attr_indices\n",
      "\n",
      "\n",
      "def save_training_visualizations(vae, clip_consistency, vis_batch, epoch, save_dir):\n",
      "    \"\"\"\n",
      "    Generate and save training visualizations\n",
      "    \"\"\"\n",
      "    images, attrs = vis_batch\n",
      "    images = images.to(vae.encoder.conv1.weight.device)\n",
      "    attrs = attrs.to(vae.encoder.conv1.weight.device)\n",
      "\n",
      "    # Generate samples\n",
      "    recon_images, perturbed_images, perturbed_attrs, attr_indices = generate_perturbation_samples(\n",
      "        vae, clip_consistency, images, attrs, images.device\n",
      "    )\n",
      "\n",
      "    # Plot results\n",
      "    plot_reconstructions_with_perturbations(\n",
      "        images, recon_images, perturbed_images,\n",
      "        attrs, perturbed_attrs, attr_indices, epoch, save_dir  # Add save_dir here\n",
      "    )\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad5b344db8aa9341"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
