{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T12:32:30.335460Z",
     "start_time": "2025-02-10T12:32:30.261295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def generate_py_file_string(directory=\".\"):\n",
    "    result = \"\"\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                    result += f\"File: {file}\\n\\n{content}\\n{'-' * 40}\\n\"\n",
    "                except Exception as e:\n",
    "                    result += f\"File: {file}\\n\\nError reading file: {e}\\n{'-' * 40}\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output = generate_py_file_string()\n",
    "    print(output)\n",
    "    "
   ],
   "id": "e3f36a820c575835",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: models.py\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "from torch import nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "from clip_loss import  CLIPAttributeConsistency\n",
      "\n",
      "\n",
      "class Encoder(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Encoder, self).__init__()\n",
      "        # channels_in ,  channels_out, kernel_size, stride , padding,\n",
      "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1)\n",
      "        self.conv2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
      "        self.conv3 = nn.Conv2d(64, 64, 4, 2, 1)\n",
      "        self.conv4 = nn.Conv2d(64, 128, 4, 2, 1)\n",
      "        self.maxp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "        self.maxp4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
      "\n",
      "    def forward(self, x):\n",
      "        out = self.conv1(x)\n",
      "        out = self.maxp1(out)\n",
      "        out = F.relu(out)\n",
      "        out = self.conv2(out)\n",
      "        out = self.maxp2(out)\n",
      "        out = F.relu(out)\n",
      "        out = self.conv3(out)\n",
      "        out = self.maxp3(out)\n",
      "        out = F.relu(out)\n",
      "        out = self.conv4(out)\n",
      "        out = self.maxp4(out)\n",
      "        out = F.relu(out)\n",
      "        return out.view(out.shape[0], -1)\n",
      "\n",
      "\n",
      "class Decoder(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Decoder, self).__init__()\n",
      "        # channels_in ,  channels_out, kernel_size, stride , padding,\n",
      "        self.transconv1 = nn.ConvTranspose2d(64 + 40, 64, 8, 4, 2)\n",
      "        self.transconv2 = nn.ConvTranspose2d(64, 64, 8, 4, 2)\n",
      "        self.transconv3 = nn.ConvTranspose2d(64, 64, 4, 2, 1)\n",
      "        self.transconv4 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
      "\n",
      "\n",
      "        self.hairEmbedding = nn.Embedding(4, 10)\n",
      "        self.beardEmbedding = nn.Embedding(2, 10)\n",
      "        self.genderEmbedding = nn.Embedding(2, 10)\n",
      "        self.paleSkinEmbedding = nn.Embedding(2, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        z = x[:, :64]\n",
      "        hair = self.hairEmbedding(x[:, 64].long())\n",
      "        paleSkin = self.paleSkinEmbedding(x[:, 65].long())\n",
      "        gender = self.genderEmbedding(x[:, 66].long())\n",
      "        beard = self.beardEmbedding(x[:, 67].long())\n",
      "        \"\"\"\n",
      "        Concating the embeddings and the encoded image\n",
      "        \"\"\"\n",
      "        z = torch.cat([z, hair, beard, gender, paleSkin], dim=1)\n",
      "\n",
      "        out = self.transconv1(z.view(z.shape[0], z.shape[1], 1, 1))\n",
      "        out = F.relu(out)\n",
      "        out = self.transconv2(out)\n",
      "        out = F.relu(out)\n",
      "\n",
      "        out = self.transconv3(out)\n",
      "        out = F.relu(out)\n",
      "\n",
      "        out = self.transconv4(out)\n",
      "        out = F.sigmoid(out)\n",
      "\n",
      "        return out\n",
      "\n",
      "\n",
      "class CVAE(nn.Module):\n",
      "    def __init__(self, encoder, decoder):\n",
      "        super(CVAE, self).__init__()\n",
      "        self.encoder = encoder()\n",
      "        self.decoder = decoder()\n",
      "\n",
      "    def forward(self, x, attrs):\n",
      "        h = self.encoder(x)\n",
      "\n",
      "        mu = h[:, :64]\n",
      "        logvar = h[:, 64:]\n",
      "        # this part is for the reparameterization trick\n",
      "        s = torch.exp(logvar)\n",
      "        eps = torch.randn_like(s)\n",
      "        z = s * eps + mu\n",
      "\n",
      "        z = torch.cat([z, attrs], dim=1)\n",
      "        out = self.decoder(z)\n",
      "        return out, mu, logvar\n",
      "\n",
      "\n",
      "def loss_function(recon_x, x, mu, logvar, vae, encoder_output, attrs,\n",
      "                      beta_vae=1.0, beta_clip=1.0, clip_consistency=None):\n",
      "    \"\"\"\n",
      "    Enhanced VAE loss function with CLIP-based attribute manipulation loss\n",
      "\n",
      "    Args:\n",
      "        recon_x: reconstructed input\n",
      "        x: original input\n",
      "        mu: mean of the latent distribution\n",
      "        logvar: log variance of the latent distribution\n",
      "        vae: VAE model\n",
      "        encoder_output: full encoder output\n",
      "        attrs: attribute tensor\n",
      "        beta_vae: weight for the KL divergence term\n",
      "        beta_clip: weight for the CLIP-based attribute loss\n",
      "        clip_consistency: CLIPAttributeConsistency instance\n",
      "    \"\"\"\n",
      "    # Original VAE losses\n",
      "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
      "    recon_loss = recon_loss / (x.size(0) * 3 * 64 * 64)\n",
      "\n",
      "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
      "    KLD = KLD / (x.size(0) * 3 * 64 * 64)\n",
      "\n",
      "    # Initialize CLIP consistency checker if not provided\n",
      "    if clip_consistency is None:\n",
      "        clip_consistency = CLIPAttributeConsistency(device=x.device)\n",
      "\n",
      "    # Compute CLIP-based attribute consistency loss\n",
      "    clip_loss = clip_consistency.compute_attribute_loss(vae, encoder_output, attrs)\n",
      "\n",
      "    # Combine all losses\n",
      "    total_loss = recon_loss + beta_vae * KLD + beta_clip * clip_loss\n",
      "\n",
      "    return total_loss, recon_loss, KLD, clip_loss\n",
      "\n",
      "\n",
      "class CosineScheduler:\n",
      "    def __init__(self, start_value, end_value, num_cycles, num_epochs):\n",
      "        \"\"\"\n",
      "        Cosine annealing scheduler for beta value\n",
      "\n",
      "        Args:\n",
      "            start_value: Initial beta value\n",
      "            end_value: Final beta value\n",
      "            num_cycles: Number of cycles for the cosine schedule\n",
      "            num_epochs: Total number of epochs\n",
      "        \"\"\"\n",
      "        self.start_value = start_value\n",
      "        self.end_value = end_value\n",
      "        self.num_cycles = num_cycles\n",
      "        self.num_epochs = num_epochs\n",
      "\n",
      "    def get_value(self, epoch):\n",
      "        \"\"\"\n",
      "        Calculate the current beta value based on the epoch\n",
      "        \"\"\"\n",
      "        # Calculate the progress within the current cycle\n",
      "        cycle_length = self.num_epochs / self.num_cycles\n",
      "        cycle_progress = (epoch % cycle_length) / cycle_length\n",
      "\n",
      "        # Calculate cosine value (0 to 1)\n",
      "        cosine_value = 0.5 * (1 + np.cos(np.pi * cycle_progress))\n",
      "\n",
      "        # Interpolate between start and end values\n",
      "        current_value = self.end_value + (self.start_value - self.end_value) * cosine_value\n",
      "        return current_value\n",
      "----------------------------------------\n",
      "File: utils.py\n",
      "\n",
      "def ceil(a,b):\n",
      "    return -(-a//b)\n",
      "\n",
      "----------------------------------------\n",
      "File: clip_loss.py\n",
      "\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "import clip\n",
      "from random import choice\n",
      "\n",
      "\n",
      "class CLIPAttributeConsistency:\n",
      "    def __init__(self, clip_model_name=\"ViT-B/32\", device=\"cuda\"):\n",
      "        self.device = device\n",
      "        self.model, self.preprocess = clip.load(clip_model_name, device=device)\n",
      "\n",
      "        self.attribute_templates = {\n",
      "            'hair_color': {\n",
      "                0: [\"a photo of a person with blonde hair\", \"a face with blonde hair\"],\n",
      "                1: [\"a photo of a person with brown hair\", \"a face with brown hair\"],\n",
      "                2: [\"a photo of a person with black hair\", \"a face with black hair\"],\n",
      "                3: [\"a photo of a person with red hair\", \"a face with red hair\"]\n",
      "            },\n",
      "            'pale_skin': {\n",
      "                0: [\"a photo of a person with dark skin\", \"a dark-skinned person\"],\n",
      "                1: [\"a photo of a person with pale skin\", \"a pale-skinned person\"]\n",
      "            },\n",
      "            'gender': {\n",
      "                0: [\"a photo of a woman\", \"a female face\"],\n",
      "                1: [\"a photo of a man\", \"a male face\"]\n",
      "            },\n",
      "            'beard': {\n",
      "                0: [\"a photo of a person without a beard\", \"a face without facial hair\"],\n",
      "                1: [\"a photo of a person with a beard\", \"a face with a beard\"]\n",
      "            }\n",
      "        }\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def get_image_features(self, images):\n",
      "        \"\"\"Get normalized CLIP image features\"\"\"\n",
      "        images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
      "        image_features = self.model.encode_image(images)\n",
      "        return image_features / image_features.norm(dim=-1, keepdim=True)\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def get_text_features(self, descriptions):\n",
      "        \"\"\"Get normalized CLIP text features for a list of descriptions\"\"\"\n",
      "        text_tokens = clip.tokenize(descriptions).to(self.device)\n",
      "        text_features = self.model.encode_text(text_tokens)\n",
      "        return text_features / text_features.norm(dim=-1, keepdim=True)\n",
      "\n",
      "    def compute_attribute_loss(self, vae, encoder_output, attrs):\n",
      "        \"\"\"\n",
      "        Compute CLIP-based attribute consistency loss for entire batch in parallel\n",
      "        \"\"\"\n",
      "        batch_size = attrs.size(0)\n",
      "\n",
      "        # Get latent representations\n",
      "        z = encoder_output[:, :64]  # [batch_size, 64]\n",
      "\n",
      "        # Randomly select attributes to manipulate for each image in batch\n",
      "        attr_indices = torch.randint(0, 4, (batch_size,), device=attrs.device)  # [batch_size]\n",
      "\n",
      "        # Create mask for multiclass (hair_color) vs binary attributes\n",
      "        is_hair_color = (attr_indices == 0)\n",
      "\n",
      "        # Get original attribute values\n",
      "        orig_attr_vals = torch.gather(attrs, 1, attr_indices.unsqueeze(1)).squeeze(1)  # [batch_size]\n",
      "\n",
      "        # Create perturbed attributes tensor\n",
      "        perturbed_attrs = attrs.clone().float()  # [batch_size, num_attrs]\n",
      "\n",
      "        # Handle hair color (4 classes)\n",
      "        hair_mask = is_hair_color\n",
      "        if hair_mask.any():\n",
      "            # For hair color, randomly select a different class\n",
      "            new_hair_vals = torch.randint(0, 4, (hair_mask.sum(),), device=attrs.device).float()\n",
      "            # Make sure new values are different from original\n",
      "            same_hair = new_hair_vals == orig_attr_vals[hair_mask]\n",
      "            if same_hair.any():\n",
      "                new_hair_vals[same_hair] = (new_hair_vals[same_hair] + 1) % 4\n",
      "            perturbed_attrs[hair_mask, 0] = new_hair_vals\n",
      "\n",
      "        # Handle binary attributes\n",
      "        binary_mask = ~is_hair_color\n",
      "        if binary_mask.any():\n",
      "            # For binary attributes, flip the value\n",
      "            binary_indices = attr_indices[binary_mask]\n",
      "            perturbed_attrs[binary_mask, binary_indices] = 1 - orig_attr_vals[binary_mask]\n",
      "\n",
      "        # Get reconstructions for entire batch\n",
      "        with torch.no_grad():\n",
      "            # Original reconstructions\n",
      "            orig_decoded = vae.decoder(torch.cat([z, attrs], dim=1))\n",
      "\n",
      "            # Perturbed reconstructions\n",
      "            perturbed_decoded = vae.decoder(torch.cat([z, perturbed_attrs], dim=1))\n",
      "\n",
      "            # Get CLIP embeddings for all images\n",
      "            orig_features = self.get_image_features(orig_decoded)\n",
      "            perturbed_features = self.get_image_features(perturbed_decoded)\n",
      "\n",
      "            # Initialize lists for text features\n",
      "            all_orig_text_features = []\n",
      "            all_perturbed_text_features = []\n",
      "\n",
      "            # Get text features for each attribute type\n",
      "            attr_names = ['hair_color', 'pale_skin', 'gender', 'beard']\n",
      "            for attr_type in range(4):\n",
      "                mask = (attr_indices == attr_type)\n",
      "                if not mask.any():\n",
      "                    continue\n",
      "\n",
      "                # Get original descriptions\n",
      "                orig_descriptions = []\n",
      "                for idx in range(batch_size):\n",
      "                    if mask[idx]:\n",
      "                        orig_descriptions.extend(\n",
      "                            self.attribute_templates[attr_names[attr_type]][int(orig_attr_vals[idx].item())]\n",
      "                        )\n",
      "\n",
      "                # Get perturbed descriptions\n",
      "                perturbed_descriptions = []\n",
      "                for idx in range(batch_size):\n",
      "                    if mask[idx]:\n",
      "                        perturbed_descriptions.extend(\n",
      "                            self.attribute_templates[attr_names[attr_type]][int(perturbed_attrs[idx, attr_type].item())]\n",
      "                        )\n",
      "\n",
      "                if orig_descriptions:  # If we have any descriptions for this attribute\n",
      "                    orig_text_features = self.get_text_features(orig_descriptions)\n",
      "                    perturbed_text_features = self.get_text_features(perturbed_descriptions)\n",
      "\n",
      "                    # Reshape to account for multiple descriptions per image\n",
      "                    num_desc = len(self.attribute_templates[attr_names[attr_type]][0])\n",
      "                    orig_text_features = orig_text_features.view(-1, num_desc, orig_text_features.size(-1))\n",
      "                    perturbed_text_features = perturbed_text_features.view(-1, num_desc,\n",
      "                                                                           perturbed_text_features.size(-1))\n",
      "\n",
      "                    all_orig_text_features.append((mask, orig_text_features))\n",
      "                    all_perturbed_text_features.append((mask, perturbed_text_features))\n",
      "\n",
      "            # Compute losses for each attribute type and combine\n",
      "            total_loss = 0\n",
      "            margin = 0.2\n",
      "\n",
      "            for (mask, orig_text), (_, perturbed_text) in zip(all_orig_text_features, all_perturbed_text_features):\n",
      "                if not mask.any():\n",
      "                    continue\n",
      "\n",
      "                # Compute similarities for masked batch\n",
      "                orig_features_masked = orig_features[mask]\n",
      "                perturbed_features_masked = perturbed_features[mask]\n",
      "\n",
      "                # Compute mean similarity across multiple descriptions\n",
      "                orig_sim = (orig_features_masked @ orig_text.mean(dim=1).t()).mean(dim=1)\n",
      "                perturbed_sim = (perturbed_features_masked @ perturbed_text.mean(dim=1).t()).mean(dim=1)\n",
      "\n",
      "                # Cross similarities\n",
      "                cross_orig_sim = (orig_features_masked @ perturbed_text.mean(dim=1).t()).mean(dim=1)\n",
      "                cross_perturbed_sim = (perturbed_features_masked @ orig_text.mean(dim=1).t()).mean(dim=1)\n",
      "\n",
      "                # Compute contrastive loss\n",
      "                loss = (\n",
      "                        torch.relu(cross_orig_sim - orig_sim + margin) +\n",
      "                        torch.relu(cross_perturbed_sim - perturbed_sim + margin)\n",
      "                ).mean()\n",
      "\n",
      "                total_loss += loss\n",
      "\n",
      "            return total_loss / len(all_orig_text_features)\n",
      "\n",
      "----------------------------------------\n",
      "File: celeba_project.py\n",
      "\n",
      "from PIL import Image\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import matplotlib.pyplot as plt\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "import torchvision.transforms as transforms\n",
      "import os\n",
      "from PIL import Image\n",
      "\n",
      "import torch.optim as  optim\n",
      "from tqdm import tqdm\n",
      "\n",
      "from clip_loss import CLIPAttributeConsistency\n",
      "from dataset import CelebaDataset, transform\n",
      "from models import Encoder, Decoder, CVAE, loss_function, CosineScheduler\n",
      "from plots import plot_reconstructions, save_training_visualizations\n",
      "\n",
      "if torch.cuda.is_available():\n",
      "  dev = \"cuda:0\"\n",
      "  print(\"gpu up\")\n",
      "else:\n",
      "  dev = \"cpu\"\n",
      "device = torch.device(dev)\n",
      "\n",
      "df = pd.read_csv(\"celeba_dataset/list_attr_celeba.csv\")\n",
      "\n",
      "def haircolor(x):\n",
      "    if x[\"Blond_Hair\"] == 1:\n",
      "        return 0\n",
      "    elif x[\"Brown_Hair\"] == 1:\n",
      "        return 1\n",
      "    elif x[\"Black_Hair\"] == 1:\n",
      "        return 2\n",
      "    else:\n",
      "        return 3\n",
      "\n",
      "\n",
      "df[\"Hair_Color\"] = df.apply(haircolor, axis=1)\n",
      "\n",
      "df = df[[\"image_id\",\"Hair_Color\",'Pale_Skin',\"Male\",\"No_Beard\"]]\n",
      "df.Pale_Skin = df.Pale_Skin.apply(lambda x: max(x,0))\n",
      "df.Male = df.Male.apply(lambda x: max(x,0))\n",
      "df.No_Beard = df.No_Beard.apply(lambda x: max(x,0))\n",
      "\n",
      "\n",
      "\n",
      "# Initialize dataset and dataloader\n",
      "dataset = CelebaDataset(\n",
      "    df=df,\n",
      "    img_dir = \"/home/omrid/Desktop/jungo /projectCLIPvae/celeba_dataset/img_align_celeba/img_align_celeba/\",\n",
      "    transform=transform\n",
      ")\n",
      "\n",
      "dataloader = DataLoader(\n",
      "    dataset,\n",
      "    batch_size=128,\n",
      "    shuffle=True,\n",
      "    num_workers=4,  # Parallel data loading\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "vae = CVAE(Encoder, Decoder)\n",
      "vae.to(device)\n",
      "clip_consistency = CLIPAttributeConsistency(device=device)\n",
      "\n",
      "\n",
      "def train_vae(vae, dataloader, optimizer, device, num_epochs=1201, save_interval=100):\n",
      "    \"\"\"\n",
      "    Train the VAE model using BCE loss with beta scheduling\n",
      "    \"\"\"\n",
      "    vae.train()\n",
      "    loss_history = {\n",
      "        'total': [],\n",
      "        'reconstruction': [],\n",
      "        'kl': [],\n",
      "        'beta': []\n",
      "    }\n",
      "\n",
      "    # Initialize beta scheduler\n",
      "    beta_scheduler = CosineScheduler(\n",
      "        start_value=0.0,  # Start with low KL weight\n",
      "        end_value=1.0,  # End with full KL weight\n",
      "        num_cycles=4,  # Number of cycles during training\n",
      "        num_epochs=num_epochs\n",
      "    )\n",
      "\n",
      "    for epoch in range(num_epochs):\n",
      "        pbar = tqdm(total=len(dataloader), desc=f'Epoch {epoch}')\n",
      "        epoch_total_loss = 0\n",
      "        epoch_recon_loss = 0\n",
      "        epoch_kl_loss = 0\n",
      "\n",
      "        # Get current beta value\n",
      "        current_beta = beta_scheduler.get_value(epoch)\n",
      "\n",
      "        # Store first batch for visualization\n",
      "        vis_batch = None\n",
      "\n",
      "        for batch_idx, (images, attrs) in enumerate(dataloader):\n",
      "            if batch_idx == 0:\n",
      "                vis_batch = (images[:5].clone(), attrs[:5].clone())\n",
      "\n",
      "            images = images.to(device)\n",
      "            attrs = attrs.to(device)\n",
      "\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            encoder_output = vae.encoder(images)\n",
      "            recon_images, mu, logvar = vae(images, attrs)\n",
      "\n",
      "            # Compute losses with current beta value\n",
      "            total_loss, recon_loss, kl_loss, clip_loss = loss_function(\n",
      "                recon_images,\n",
      "                images,\n",
      "                mu,\n",
      "                logvar,\n",
      "                vae,\n",
      "                encoder_output,\n",
      "                attrs,\n",
      "                beta_vae=1.0,\n",
      "                beta_clip=0.2,\n",
      "                clip_consistency=clip_consistency  # Pass the initialized checker\n",
      "            )\n",
      "\n",
      "            total_loss.backward()\n",
      "            optimizer.step()\n",
      "\n",
      "            # Accumulate losses\n",
      "            epoch_total_loss += total_loss.item()\n",
      "            epoch_recon_loss += recon_loss.item()\n",
      "            epoch_kl_loss += kl_loss.item()\n",
      "\n",
      "            pbar.update(1)\n",
      "\n",
      "        pbar.close()\n",
      "\n",
      "        # Calculate averages\n",
      "        avg_total = epoch_total_loss / len(dataloader)\n",
      "        avg_recon = epoch_recon_loss / len(dataloader)\n",
      "        avg_kl = epoch_kl_loss / len(dataloader)\n",
      "\n",
      "        # Store in history\n",
      "        loss_history['total'].append(avg_total)\n",
      "        loss_history['reconstruction'].append(avg_recon)\n",
      "        loss_history['kl'].append(avg_kl)\n",
      "        loss_history['beta'].append(current_beta)\n",
      "\n",
      "        print(f\"\\nEpoch {epoch} Summary:\")\n",
      "        print(f\"Total Loss: {avg_total:.6f}\")\n",
      "        print(f\"BCE Loss: {avg_recon:.6f}\")\n",
      "        print(f\"KL Loss: {avg_kl:.6f}\")\n",
      "        print(f\"Current Beta: {current_beta:.6f}\\n\")\n",
      "\n",
      "        # Generate and save reconstructions\n",
      "        if vis_batch is not None:\n",
      "            save_training_visualizations(vae, clip_consistency, vis_batch, epoch)\n",
      "\n",
      "        # Save checkpoint at intervals\n",
      "        if epoch % save_interval == 0:\n",
      "            torch.save({\n",
      "                'epoch': epoch,\n",
      "                'model_state_dict': vae.state_dict(),\n",
      "                'optimizer_state_dict': optimizer.state_dict(),\n",
      "                'loss': avg_total,\n",
      "                'beta': current_beta\n",
      "            }, f\"vae_checkpoint_epoch_{epoch}.pt\")\n",
      "\n",
      "    return loss_history\n",
      "\n",
      "\n",
      "# Setup training\n",
      "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "vae = vae.to(device)\n",
      "optimizer =  optim.Adam(vae.parameters(), lr=0.0001)  # Adjust learning rate if needed\n",
      "\n",
      "# Train the model\n",
      "loss_history = train_vae(\n",
      "    vae=vae,\n",
      "    dataloader=dataloader,\n",
      "    optimizer=optimizer,\n",
      "    device=device,\n",
      "    num_epochs=1201,\n",
      "    save_interval=100\n",
      ")\n",
      "# Plot training progress\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(loss_history)\n",
      "plt.title('Training Loss Over Time')\n",
      "plt.xlabel('Epoch')\n",
      "plt.ylabel('Loss')\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "----------------------------------------\n",
      "File: dataset.py\n",
      "\n",
      "import os\n",
      "\n",
      "import torch\n",
      "from PIL import Image\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "import torchvision.transforms as transforms\n",
      "\n",
      "\n",
      "class CelebaDataset(Dataset):\n",
      "    def __init__(self, df, img_dir, transform=None):\n",
      "        self.df = df\n",
      "        self.img_dir = img_dir\n",
      "        self.transform = transform\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.df)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        img_name = self.df.iloc[idx]['image_id']\n",
      "        img_path = os.path.join(self.img_dir, img_name)\n",
      "        image = Image.open(img_path)\n",
      "\n",
      "        if self.transform:\n",
      "            image = self.transform(image)\n",
      "\n",
      "        # Extract attributes\n",
      "        attrs = self.df.iloc[idx][['Hair_Color', 'Pale_Skin', 'Male', 'No_Beard']].values\n",
      "        attrs = torch.tensor(attrs.astype('float32'))\n",
      "\n",
      "        return image, attrs\n",
      "\n",
      "\n",
      "# Define transforms\n",
      "transform = transforms.Compose([\n",
      "    transforms.Resize((64, 64)),\n",
      "    transforms.ToTensor(),  # Automatically scales to [0,1] and converts to (C, H, W)\n",
      "])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "File: clip_classifier.py\n",
      "\n",
      "import torch\n",
      "import clip\n",
      "from PIL import Image\n",
      "import pandas as pd\n",
      "from tqdm import tqdm\n",
      "import os\n",
      "from typing import Dict, List, Union\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class CustomAttributeClassifier:\n",
      "    def __init__(self, clip_model_name: str = \"ViT-B/32\", device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
      "        self.device = device\n",
      "        print(f\"Initializing CLIP model on {device}\")\n",
      "        self.model, self.preprocess = clip.load(clip_model_name, device=device)\n",
      "\n",
      "    def create_text_templates(self, attribute_values: Dict[str, List[str]]) -> Dict[str, Dict[str, List[str]]]:\n",
      "        templates = {}\n",
      "        for attr, values in attribute_values.items():\n",
      "            templates[attr] = {}\n",
      "            for idx, value in enumerate(values):\n",
      "                templates[attr][idx] = [\n",
      "                    f\"a photo of a person with {value} appearance\",\n",
      "                    f\"a {value} person\",\n",
      "                    f\"a face with {value} features\"\n",
      "                ]\n",
      "        return templates\n",
      "\n",
      "    def classify_images(self,\n",
      "                        img_dir: str,\n",
      "                        attribute_values: Dict[str, List[str]],\n",
      "                        batch_size: int = 8) -> pd.DataFrame:\n",
      "        \"\"\"\n",
      "        Classify images according to specified attributes using CLIP\n",
      "        \"\"\"\n",
      "        # Verify directory exists\n",
      "        if not os.path.exists(img_dir):\n",
      "            raise ValueError(f\"Directory not found: {img_dir}\")\n",
      "\n",
      "        # Create text templates\n",
      "        templates = self.create_text_templates(attribute_values)\n",
      "\n",
      "        # Get list of image files\n",
      "        print(f\"Scanning directory: {img_dir}\")\n",
      "        image_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
      "\n",
      "        if not image_files:\n",
      "            raise ValueError(f\"No image files found in {img_dir}\")\n",
      "\n",
      "        print(f\"Found {len(image_files)} images\")\n",
      "\n",
      "        # Initialize results dictionary\n",
      "        results = {\n",
      "            'image_id': []\n",
      "        }\n",
      "\n",
      "        # Add columns for each attribute\n",
      "        for attr in attribute_values.keys():\n",
      "            results[attr] = []\n",
      "\n",
      "        # Process images in batches\n",
      "        for i in tqdm(range(0, len(image_files), batch_size), desc=\"Classifying images\"):\n",
      "            batch_files = image_files[i:i + batch_size]\n",
      "            batch_images = []\n",
      "            valid_files = []\n",
      "\n",
      "            # Load and preprocess images\n",
      "            for img_file in batch_files:\n",
      "                try:\n",
      "                    img_path = os.path.join(img_dir, img_file)\n",
      "                    image = Image.open(img_path).convert('RGB')  # Ensure RGB format\n",
      "                    processed_image = self.preprocess(image)\n",
      "                    batch_images.append(processed_image)\n",
      "                    valid_files.append(img_file)\n",
      "                except Exception as e:\n",
      "                    print(f\"Error processing {img_file}: {str(e)}\")\n",
      "                    continue\n",
      "\n",
      "            if not batch_images:\n",
      "                continue\n",
      "\n",
      "            # Stack images into a batch\n",
      "            try:\n",
      "                image_batch = torch.stack(batch_images).to(self.device)\n",
      "\n",
      "                # Process each attribute\n",
      "                with torch.no_grad():\n",
      "                    image_features = self.model.encode_image(image_batch)\n",
      "                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
      "\n",
      "                    for attr, values in attribute_values.items():\n",
      "                        # Prepare text features for all templates of this attribute\n",
      "                        all_text_features = []\n",
      "                        for idx in range(len(values)):\n",
      "                            texts = templates[attr][idx]\n",
      "                            text_tokens = clip.tokenize(texts).to(self.device)\n",
      "                            text_features = self.model.encode_text(text_tokens)\n",
      "                            text_features = text_features.mean(dim=0, keepdim=True)\n",
      "                            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
      "                            all_text_features.append(text_features)\n",
      "\n",
      "                        text_features = torch.cat(all_text_features)\n",
      "                        similarity = (100.0 * image_features @ text_features.T)\n",
      "                        predictions = similarity.softmax(dim=-1).cpu().numpy()\n",
      "                        class_indices = predictions.argmax(axis=1)\n",
      "\n",
      "                        if attr not in results:\n",
      "                            results[attr] = []\n",
      "                        results[attr].extend(class_indices.tolist())\n",
      "\n",
      "                results['image_id'].extend(valid_files)\n",
      "\n",
      "            except Exception as e:\n",
      "                print(f\"Error processing batch: {str(e)}\")\n",
      "                continue\n",
      "\n",
      "        # Create DataFrame\n",
      "        df = pd.DataFrame(results)\n",
      "        print(f\"\\nProcessed {len(df)} images successfully\")\n",
      "        print(\"\\nAttribute distribution:\")\n",
      "        for attr in attribute_values.keys():\n",
      "            print(f\"\\n{attr}:\")\n",
      "            print(df[attr].value_counts())\n",
      "\n",
      "        return df\n",
      "\n",
      "\n",
      "def prepare_attributes_for_vae(df: pd.DataFrame,\n",
      "                               attribute_mapping: Dict[str, List[str]]) -> pd.DataFrame:\n",
      "    \"\"\"\n",
      "    Prepare the classified attributes for use in the VAE\n",
      "\n",
      "    Args:\n",
      "        df: DataFrame with CLIP classifications\n",
      "        attribute_mapping: Dictionary mapping attribute names to their possible values\n",
      "\n",
      "    Returns:\n",
      "        DataFrame with attributes encoded for VAE\n",
      "    \"\"\"\n",
      "    vae_df = df.copy()\n",
      "\n",
      "    # Convert categorical variables to numeric\n",
      "    for attr, values in attribute_mapping.items():\n",
      "        vae_df[attr] = vae_df[attr].astype(int)\n",
      "\n",
      "    return vae_df\n",
      "----------------------------------------\n",
      "File: plots.py\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "\n",
      "def plot_reconstructions(original_images, reconstructed_images, attrs, epoch):\n",
      "    \"\"\"\n",
      "    Plot 5 original and reconstructed images side by side with their attributes\n",
      "    \"\"\"\n",
      "    fig, axes = plt.subplots(5, 2, figsize=(12, 15))\n",
      "    fig.suptitle(f'Original vs Reconstructed Images - Epoch {epoch}')\n",
      "\n",
      "    # Labels for attributes\n",
      "    hair_labels = ['Blond', 'Brown', 'Black', 'Other']\n",
      "\n",
      "    for idx in range(5):\n",
      "        if idx < len(original_images):\n",
      "            # Original image\n",
      "            orig_img = original_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 0].imshow(orig_img)\n",
      "            axes[idx, 0].set_title('Original')\n",
      "\n",
      "            # Reconstructed image\n",
      "            recon_img = reconstructed_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 1].imshow(recon_img)\n",
      "            axes[idx, 1].set_title('Reconstructed')\n",
      "\n",
      "            # Get attributes for this image\n",
      "            attr = attrs[idx]\n",
      "            attr_text = f'Hair: {hair_labels[int(attr[0])]} | '\n",
      "            attr_text += f'Pale Skin: {bool(attr[1])} | '\n",
      "            attr_text += f'Male: {bool(attr[2])} | '\n",
      "            attr_text += f'No Beard: {bool(attr[3])}'\n",
      "\n",
      "            # Add attributes as text below the images\n",
      "            plt.figtext(0.5, 0.98 - (idx * 0.2), attr_text,\n",
      "                        ha='center', va='top', bbox=dict(facecolor='white', alpha=0.8))\n",
      "\n",
      "            # Remove axes\n",
      "            axes[idx, 0].axis('off')\n",
      "            axes[idx, 1].axis('off')\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(f'/home/omrid/Desktop/jungo /projectCLIPvae/plot_reconstructions/reconstructions_epoch_{epoch}.png')\n",
      "    plt.close()\n",
      "\n",
      "\n",
      "def plot_training_progress(loss_history):\n",
      "    \"\"\"\n",
      "    Plot training progress including losses and beta values\n",
      "    \"\"\"\n",
      "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
      "\n",
      "    # Plot losses\n",
      "    ax1.plot(loss_history['total'], label='Total Loss')\n",
      "    ax1.plot(loss_history['reconstruction'], label='Reconstruction Loss')\n",
      "    ax1.plot(loss_history['kl'], label='KL Loss')\n",
      "    ax1.set_title('Training Losses Over Time')\n",
      "    ax1.set_xlabel('Epoch')\n",
      "    ax1.set_ylabel('Loss')\n",
      "    ax1.grid(True)\n",
      "    ax1.legend()\n",
      "\n",
      "    # Plot beta values\n",
      "    ax2.plot(loss_history['beta'], label='Beta Value', color='purple')\n",
      "    ax2.set_title('Beta Schedule Over Time')\n",
      "    ax2.set_xlabel('Epoch')\n",
      "    ax2.set_ylabel('Beta')\n",
      "    ax2.grid(True)\n",
      "    ax2.legend()\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import torch\n",
      "\n",
      "\n",
      "def plot_reconstructions_with_perturbations(original_images, reconstructed_images,\n",
      "                                            perturbed_images, orig_attrs, perturbed_attrs,\n",
      "                                            attr_indices, epoch):\n",
      "    \"\"\"\n",
      "    Plot original, reconstructed, and perturbed images side by side\n",
      "\n",
      "    Args:\n",
      "        original_images: Original input images\n",
      "        reconstructed_images: VAE reconstructed images\n",
      "        perturbed_images: Images with perturbed attributes\n",
      "        orig_attrs: Original attributes\n",
      "        perturbed_attrs: Perturbed attributes\n",
      "        attr_indices: Which attribute was modified for each image\n",
      "        epoch: Current training epoch\n",
      "    \"\"\"\n",
      "    num_samples = min(5, len(original_images))\n",
      "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 4 * num_samples))\n",
      "    fig.suptitle(f'Original vs Reconstructed vs Perturbed Images - Epoch {epoch}')\n",
      "\n",
      "    # Labels for attributes\n",
      "    attr_names = ['Hair Color', 'Pale Skin', 'Gender', 'No Beard']\n",
      "    hair_labels = ['Blonde', 'Brown', 'Black', 'Other']\n",
      "    binary_labels = {\n",
      "        'Pale Skin': ['Dark', 'Pale'],\n",
      "        'Gender': ['Female', 'Male'],\n",
      "        'No Beard': ['Has Beard', 'No Beard']\n",
      "    }\n",
      "\n",
      "    for idx in range(num_samples):\n",
      "        if idx < len(original_images):\n",
      "            # Original image\n",
      "            orig_img = original_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 0].imshow(orig_img)\n",
      "            axes[idx, 0].set_title('Original')\n",
      "\n",
      "            # Reconstructed image\n",
      "            recon_img = reconstructed_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 1].imshow(recon_img)\n",
      "            axes[idx, 1].set_title('Reconstructed')\n",
      "\n",
      "            # Perturbed image\n",
      "            perturbed_img = perturbed_images[idx].cpu().permute(1, 2, 0).detach().numpy()\n",
      "            axes[idx, 2].imshow(perturbed_img)\n",
      "\n",
      "            # Get perturbed attribute info\n",
      "            attr_idx = attr_indices[idx].item()\n",
      "            attr_name = attr_names[attr_idx]\n",
      "\n",
      "            if attr_idx == 0:  # Hair color\n",
      "                orig_val = hair_labels[int(orig_attrs[idx, attr_idx].item())]\n",
      "                new_val = hair_labels[int(perturbed_attrs[idx, attr_idx].item())]\n",
      "            else:  # Binary attributes\n",
      "                orig_val = binary_labels[attr_name][int(orig_attrs[idx, attr_idx].item())]\n",
      "                new_val = binary_labels[attr_name][int(perturbed_attrs[idx, attr_idx].item())]\n",
      "\n",
      "            axes[idx, 2].set_title(f'Perturbed: {attr_name}\\n{orig_val} â†’ {new_val}')\n",
      "\n",
      "            # Remove axes\n",
      "            for ax in axes[idx]:\n",
      "                ax.axis('off')\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(f'plot_reconstructions/reconstructions_epoch_{epoch}.png')\n",
      "    plt.close()\n",
      "\n",
      "\n",
      "def generate_perturbation_samples(vae, clip_consistency, images, attrs, device):\n",
      "    \"\"\"\n",
      "    Generate samples with perturbed attributes for visualization\n",
      "    \"\"\"\n",
      "    vae.eval()\n",
      "    with torch.no_grad():\n",
      "        # Get encoder output\n",
      "        encoder_output = vae.encoder(images)\n",
      "        z = encoder_output[:, :64]\n",
      "\n",
      "        # Get original reconstructions\n",
      "        recon_images = vae.decoder(torch.cat([z, attrs], dim=1))\n",
      "\n",
      "        # Create perturbed attributes\n",
      "        batch_size = attrs.size(0)\n",
      "        attr_indices = torch.randint(0, 4, (batch_size,), device=device)\n",
      "\n",
      "        # Create perturbed attributes following same logic as in clip loss\n",
      "        perturbed_attrs = attrs.clone().float()\n",
      "\n",
      "        # Handle hair color (4 classes)\n",
      "        hair_mask = (attr_indices == 0)\n",
      "        if hair_mask.any():\n",
      "            new_hair_vals = torch.randint(0, 4, (hair_mask.sum(),), device=device).float()\n",
      "            same_hair = new_hair_vals == attrs[hair_mask, 0]\n",
      "            if same_hair.any():\n",
      "                new_hair_vals[same_hair] = (new_hair_vals[same_hair] + 1) % 4\n",
      "            perturbed_attrs[hair_mask, 0] = new_hair_vals\n",
      "\n",
      "        # Handle binary attributes\n",
      "        binary_mask = ~hair_mask\n",
      "        if binary_mask.any():\n",
      "            binary_indices = attr_indices[binary_mask]\n",
      "            orig_vals = torch.gather(attrs[binary_mask], 1, binary_indices.unsqueeze(1)).squeeze(1)\n",
      "            perturbed_attrs[binary_mask, binary_indices] = 1 - orig_vals\n",
      "\n",
      "        # Generate perturbed reconstructions\n",
      "        perturbed_images = vae.decoder(torch.cat([z, perturbed_attrs], dim=1))\n",
      "\n",
      "        return recon_images, perturbed_images, perturbed_attrs, attr_indices\n",
      "\n",
      "\n",
      "def save_training_visualizations(vae, clip_consistency, vis_batch, epoch):\n",
      "    \"\"\"\n",
      "    Generate and save training visualizations\n",
      "    \"\"\"\n",
      "    images, attrs = vis_batch\n",
      "    images = images.to(vae.encoder.conv1.weight.device)\n",
      "    attrs = attrs.to(vae.encoder.conv1.weight.device)\n",
      "\n",
      "    # Generate samples\n",
      "    recon_images, perturbed_images, perturbed_attrs, attr_indices = generate_perturbation_samples(\n",
      "        vae, clip_consistency, images, attrs, images.device\n",
      "    )\n",
      "\n",
      "    # Plot results\n",
      "    plot_reconstructions_with_perturbations(\n",
      "        images, recon_images, perturbed_images,\n",
      "        attrs, perturbed_attrs, attr_indices, epoch\n",
      "    )\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad5b344db8aa9341"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
